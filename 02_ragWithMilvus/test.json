[{"docId":"article_ca0a4317-08b9-423e-831f-19350c65733e","title":"不止会动嘴，还会「思考」！字节跳动发布OmniHuman-1.5，让虚拟人拥有逻辑灵魂","link":"http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650989460&idx=2&sn=4bed72a8fb1ddab60449559681be6383&chksm=859f1cd2493028308f386acdc22d1e1e789f702e1aaa5a7f8409e200b6e312b0d871181ea1ff&scene=0#rd","pubDate":"2025.09.05 15:11:00","pubAuthor":"机器之心","content":"!图片\n\n想象一个虚拟人，他不仅能精准地对上你的口型，还能在你讲到关键点时做出恍然大悟的表情，在你讲述悲伤故事时流露出同情的神态，甚至能根据你的话语逻辑做出有意义的手势。\n\n这不再是科幻电影的场景。8 月底，字节跳动数字人团队推出了 OmniHuman-1.5，提出了一种全新的虚拟人生成框架，让虚拟人真正拥有了「思考」和 「表达」的能力。\n\n数月前 OmniHuman-1 上线时，曾引发国内外热潮。相比前作，1.5 版本有了更多突破，不仅可以根据文字指令让虚拟人在对口型之余做出指定动作、表情，还支持在多人场景中控制发言者以外的角色做出具体动作。据悉，新版本很快也将上线即梦 AI。\n\n 论文链接： https://arxiv.org/abs/2508.19209\n 项目主页： https://omnihuman-lab.github.io/v1\\5/\n\n一个「会思考」的虚拟人是什么样？\n\n传统虚拟人总感觉差了点「灵魂」，动作机械、重复，而 OmniHuman-1.5 首次将诺贝尔奖得主丹尼尔・卡尼曼的「双系统理论」引入 AI，通过一个由多模态大语言模型（MLLM）驱动的「思考大脑」，让虚拟人学会了深思熟虑。\n\n在深入技术细节之前，先用最直观的方式，感受一下这个框架创造出的虚拟人，究竟有何不同：\n\n超越简单的模仿，模型展现了逻辑推理能力。它能准确理解指令，按顺序拿出红蓝药丸，执行复杂的动作意图。\n\n虚拟人精准地根据语音内容规划动作，实现了「先画眼线，再介绍眼影盘」这样的逻辑序列，展现了对内容的理解。\n\n挑战长视频与多人互动。模型不仅能生成稳定的长时间双人对唱，还能驾驭丰富的运镜效果，同时角色的动作、表情和互动极为多样，告别了单调重复。\n\n虚拟人学会了「倾听」。它可以在对话和倾听状态间自如切换，说话时的情绪与内容匹配。\n\n除了高动态场景，还是需要细腻情感表达的独白，模型都能拿捏，展现出了表演张力。\n\n双系统框架为虚拟人装上「大脑」\n\n近年来，视频虚拟人技术发展迅猛，从最初的口型合成，进化到了半身乃至全身的动画生成。大家的目标也越来越宏大：创造一个与真人无异，既能理性行动又能真实表达情感的「数字生命」。\n\n然而，尽管现有方法（尤其是基于 Diffusion Transformer 的模型）能够生成与音频同步的流畅视频，但它们更像一个出色的「反应机器」。仔细观察你会发现，这些模型捕捉到的仅仅是音频信号与身体动作之间的浅层、直接关联。结果就是，虚拟人能精准地对上口型，做一些简单的、跟随节奏的摆动，但一旦涉及更复杂的、需要理解对话内容的交互，就立刻「露馅」了。它们的行为缺乏长期规划和逻辑一致性，离真正的「以假乱真」还有很长的路要走。\n\n为什么会这样？研究者们从认知科学中找到了答案。人类的行为被认为由两个系统主导：\n\n 系统 1（System 1）： 快速、无意识、自动化的反应系统。对于虚拟人而言，这就像是驱动嘴部肌肉发出声音，或下意识的身体摇晃。这与当前模型的工作模式非常相似。\n\n 系统 2（System 2）： 缓慢、有意识、需要努力的分析系统。这对应着根据对话内容，组织一个有意义且契合语境的表情或手势。这是当前模型普遍缺乏的能力。\n\n显然，要让虚拟人「活」起来，就必须为它装上「系统 2」这个深思熟虑的大脑。因此，本文的核心思路应运而生：利用多模态大语言模型（MLLM）强大的推理能力来显式地模拟「系统 2」的决策过程，并将其与模拟「系统 1」的反应式生成模块相结合。\n\n为了实现这一构想，研究者们设计了一个精巧的「双系统模拟框架」。它主要由两部分构成：一个负责规划的「系统 2」大脑，和一个负责渲染的「系统 1」身体。\n\n图注： 框架流程图。左侧为总体流程，展示了「系统 2」如何利用 MLLM 智能体对所有输入（音、图、文）进行推理，生成一个宏观的「行为规划表」（Schedule）。这个规划表随后指导「系统 1」的 MMDiT 网络，后者在其专用的文本、音频和视频分支中融合信息，最终合成视频。右侧是关键模块的细节图。\n\n1. 系统 2：MLLM 智能体进行深思熟虑的规划\n\n这部分是整个框架的「大脑」和「指挥中心」。研究者设计了一个由两个 MLLM 组成的智能体（Agent）推理流程：\n\n 分析器（Analyzer）： 第一个 MLLM 负责「情景分析」。它接收角色的参考图、音频、以及用户可选的文本提示，然后像一个侦探一样，分析出角色的性格、情绪、意图以及周围环境，并输出结构化的分析结果\n 规划器（Planner）： 第二个 MLLM 接收「分析器」的结论，并基于此制定一个详细的「行动计划」。这个计划被构造成一个镜头序列，为视频的每一小段都定义了角色的表情和动作。\n\n通过这种「分析 - 规划」的协作，模型得以生成一个全局一致、逻辑连贯的行动计划，为虚拟人的行为提供了「顶层设计」。\n\n2. 系统 1：多模态融合网络进行反应式渲染\n\n有了「大脑」的规划，还需要一个强大的「身体」来执行。这部分由一个特殊设计的多模态扩散模型（MMDiT）承担，它负责将「系统 2」的高层文本规划与「系统 1」的底层音频信号（用于口型同步等）完美融合，生成最终视频。\n\n然而，将文本、音频、参考图这几种完全不同的信息（模态）塞进一个模型里，极易引发「模态冲突」，导致模型顾此失彼。为此，研究者提出了两大核心技术创新来解决这个难题。\n\n如何让「大脑」与「身体」高效协作？\n\n1. 重新思考身份维持：「伪最终帧」的设计\n\n传统方法为了让虚拟人保持固定的身份（长相），通常会在模型中输入一张参考图。但研究者敏锐地发现，这会带来一个严重的问题：模型会错误地学习到「生成的视频里必须出现和参考图一模一样的画面」，这极大地限制了角色的动态范围，导致动作僵硬。\n\n图注： 该图解释了为什么需要 “伪最终帧”。右侧揭示了核心困境：当参考图与目标片段内容高度相关时（绿色区域），会限制动作多样性；而当二者不相关时（红色区域），又会导致生成内容与参考图出现预期外的偏差。\n\n为此，他们提出了一个名为伪最终帧（Pseudo Last Frame）的解决方案。\n\n 训练时： 完全抛弃参考图。模型只学习根据视频的「第一帧」和「最后一帧」 来进行预测。\n\n 推理时： 将用户提供的参考图巧妙地放在「最后一帧」的位置上，并告诉模型这是一个「伪」的最终帧。\n\n这个「伪最终帧」就像一根「挂在驴子眼前的胡萝卜」：它引导着模型朝参考图的身份特征生成，但从不强迫模型必须一模一样地复现它。实验证明，这种方法完美地在「身份一致性」和「动作多样性」之间取得了平衡。\n\n2. 解决模态冲突：「对称融合」与「两阶段预热」\n\n为了让文本（系统 2 规划）和音频（系统 1 信号）更好地协作，研究者为音频信号也设计了一个独立的、与视频和文本分支结构对称的「音频分支」。这三个分支在模型的每一层都通过共享的自注意力机制进行深度融合，确保信息充分对齐。\n\n但新的问题来了：音频信号在时间上非常密集，模型在联合训练时会偷懒，倾向于只依赖音频来做所有预测，从而忽略了文本提供的高层语义指导。这就是「模态冲突」。\n\n研究者的解决方案是「两阶段预热（Two-stage Warm-up）」训练策略：\n\n 第一阶段： 先在一个「小模型」上强制让三个分支一起工作。这逼迫模型学会 「分工」：文本和视频分支负责宏观语义，音频分支则专注于自己的核心任务（如口型、语音风格）。\n\n 第二阶段： 将预训练好的主模型（文本和视频分支）与第一阶段「预热」过的音频分支组合起来，再进行微调。\n\n通过这种方式，每个分支都带着自己最擅长的「先验知识」进入最终的训练，从而有效避免了模态冲突，让「大脑」的指令和「身体」的反应都能得到忠实执行。\n\n效果对比\n\n除了直观的效果展示，硬核的量化数据和直接的 SOTA 对比更能说明问题。\n\n1.Agent 推理 + MMDiT 架构的有效性验证\n\n图注： 消融实验（Ablation Study）的结果清晰地证明了框架中两大核心设计的有效性。从数据中可以看到，无论是负责 “思考” 的 Agent 推理模块，还是负责 “执行” 的 MMDiT 架构，都对最终的生成质量，尤其是在逻辑性和语义连贯性上，做出了不可或缺的贡献。\n\n2. 全面超越 SOTA 模型\n\n图注： 在与当前最先进（SOTA）的多个公开模型进行的全方位对比中，本方法在所有关键指标上都取得了显著优势或极具竞争力的表现。\n\n图注： 这张可视化对比图直观地展示了「思考能力」的价值。相比于没有推理能力加持、只会做简单说话和重复性动作的模型方案，OmniHuman-1.5 显示了更高的动态范围和更有逻辑性的动作效果，实现了从「动嘴」到 「表达」的飞跃。\n\n总结与展望\n\nOmnihuman-1.5 为虚拟人领域提供了一个全新的、极具启发性的视角。它通过借鉴认知科学的「双系统理论」，巧妙地利用 MLLM 作为「系统 2」的推理核心，并设计了一套创新的多模态融合架构来解决关键的技术瓶颈，最终实现了虚拟人行为从「反应式」到「思考式」的飞跃。\n\n目前即梦 AI 视频生成中对口型能力的大师模式是基于 Omnihuaman-1.0，依靠一张图 + 一段音频就能生成流畅自然的虚拟人视频。很快 OmniHuman-1.5 也将上线即梦 AI。相比 1.0 版本，Omnihuaman-1.5 不仅可以生成更加真实、灵动的虚拟人，也为人机交互、影视制作、虚拟社交等领域带来新的可能。\n\n© THE END \n\n转载请联系本公众号获得授权\n\n投稿或寻求报道：liyazhou@jiqizhixin.com\n\n文章原文"},{"docId":"article_9a6cb613-c3d4-43d1-82c1-412f5d3fb1e2","title":"被网友逼着改名的谷歌Nano Banana，正在抢99%时尚博主的饭碗","link":"http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650989460&idx=1&sn=92a3c7f23bbe214cf09db6a85f6e7a4c&chksm=85b271eccf553695a971c1d6194d2d23e43ee00eb66a98b4f1ccca9843db36a2d63c90e97085&scene=0#rd","pubDate":"2025.09.05 15:11:00","pubAuthor":"机器之心","content":"机器之心报道\n\n编辑：杨文\n谷歌听劝。\n\n上周，谷歌给 Nano Banana 改了个正儿八经的名字，网友一片哗然，疯狂吐槽新名字 Gemini 2.5 Flash Image 又长又无聊，完全没有记忆点。\n\n好在谷歌听劝。\n\n有眼尖的网友发现，谷歌已经悄悄把 AI Studio 里 Gemini 2.5 Flash Image 的名字换回了 Nano Banana。\n\n甚至还有网友提议，以后所有 AI 模型都用水果和蔬菜来命名，这样更有趣，也比那些 AI 公司一贯糟糕又拗口的命名方式要好得多。\n\n言归正传。\n\n前几天我们盘点了 Nano Banana 的七种神仙玩法，其中呼声最高的就是生成 OOTD 这一趴。\n\n左右滑动查看更多\n\n所以，今天我们索性就来一期「砸」时尚博主饭碗的整活特辑。\n\n生成明星 OOTD\n\n「OOTD」 是 Outfit of the Day 英文缩写，意思是今日穿搭。\n\n如果你经常混迹 ins、微博、小红书，就会发现明星们也很爱晒穿搭照片，倪妮、舒淇、高圆圆、钟楚曦都是出了名的私服大户。\n\n左右滑动查看更多\n\n很多时尚博主就专门收集她们的穿搭照，整理成一份份清单，方便时髦精们跟着明星学穿搭。\n\n但这是个苦力活，每一步都是耗费心力的大工程：\n\n 从大量零散的活动照、街拍图里找出清晰可用的明星造型图；\n 逐一识别衣服、鞋子、包包和配饰等单品来源；\n 在最短时间里整理清单内容，把单品的品牌名、具体型号、参考价格甚至购买渠道一一标注；\n 最后还要做视觉设计，将明星造型图和单品对照图排版在一张图里，配上简洁的说明和价格标签。\n\n而且这个圈子也是相当卷，明星造型更新速度非常快，博主必须争分夺秒抢发布，否则就会被其他账号捷足先登，失去传播价值。\n\n现在有了 Nano Banana，工作流程就简单多了。\n\n以倪妮一次活动私服为例。\n\n打开 Google AI Studio，选择 Nano Banana，上传参考图，输入提示词：\n\nGenerate a flat lay OOTD outfit image from a top-down perspective based on the uploaded reference photo, ensuring that the clothing, accessories, and shoes are replicated 1:1 from the reference.（请根据上传的参考照片，生成一张俯拍平铺的 OOTD 穿搭图，服装、配饰、鞋子需与上传的参考照片 1:1 复刻）\n\n链接：https://aistudio.google.com/\n\n为了生成结果更准确，可以多丢几张各种角度和姿势的参考图。\n\n图1和图2为参考图，图3为Nano Banana生成的OOTD\n\nNano Banana 可以精准捕捉穿搭细节，比如不对称剪裁、下摆流苏、露肩设计、酒红色长裙以及黑色尖头短靴都被准确复刻。\n\n当然也有小 bug，比如针织衫显得不够修身，金色圆形珍珠耳坠也对不上号。\n\n下面这一套 OOTD 整体生成效果也不错，只不过原图中的微喇西装裤，在生成图里被偷换成了直筒裤。\n\n左图为参考图，右图为 Nano Banana 生成的 OOTD\n\n我们还可以把提示词润色得更详细一些，生成效果也更好：\n\nFrom the uploaded reference photo, extract the outfit and recreate it as a high-quality top-down flat-lay OOTD board. Requirements:\n\n Include only the visible clothing and accessories from the reference: top, bottom, shoes, and jewelry if present.\n Keep colors, textures, and silhouettes accurate to the original outfit.\n Arrange the pieces neatly in a balanced composition: tops at the top, bottoms centered, shoes placed symmetrically below, accessories arranged to the sides.\n Use a clean, neutral background (light beige or warm fabric texture) to highlight the outfit.\n Show realistic fabric folds, natural shadows, and detailed textures.\n Present the result in a modern editorial style suitable for fashion magazines or social media posts.\n Do not add extra props, models, or logos.\n Output in high resolution with crisp edges and consistent lighting.\n\n再来个进阶版本，让它生成一张带有品牌名称标注的舒淇造型清单图。\n\n结果发现，Nano Banana 一次性生成成功概率很低：要么听不懂指令，要么就是拆解后的单品货不对版、品牌名称标注错误。\n\n输入提示词：Generate a celebrity OOTD outfit checklist by identifying and breaking down each item, including clothing and accessories. Then, create an outfit breakdown image with the brand name written below each item, and include a reference price if available (omit the price if it cannot be found).Nano Banana 将礼服和配饰的品牌名称全部识别错误。\n\n我们换了个思路，分两步走：\n\n第一步，打开 Gemini 2.5，上传图片，询问「图中的礼服和珠宝分别来自哪个品牌？」Gemini 2.5 正确识别出礼服来自阿玛尼，珠宝来自宝格丽。\n\n第二步，输入提示词：请根据上传的参考照片，生成一张俯拍平铺的 OOTD 穿搭图，服装需与上传的参考照片 1:1 复刻，背景为浅粉色，并在裙子的下方写上品牌名称 “Armani Privé“，在配饰下方写上品牌名称 “BVLGARI”。\n\n虽然能实现，但流程略显繁琐。期待未来 Nano Banana 能一步到位，直接生成带有准确品牌名称标注的造型清单图。\n\n一键换衣\n\n每当看上一件好看的高定礼服，粉丝们往往第一时间就会在评论区疯狂 @ 自家明星，希望他们能穿上同款惊艳全场。\n\n于是，明星的造型师们就得绞尽脑汁，想办法去借，甚至不惜托关系、排队等候。然而，高定礼服本就难借，还存在合不合身、是否适合本人气质的现实问题。\n\n现在，有了这款「一键换衣」神器，一切就变得简单多了。\n\n我们上传一张迪丽热巴半身照和一张 AI 生成的紫色礼服图片，输入提示词：让这个人穿上上传的紫色礼服，背景换成欧式建筑前，就能立刻看到明星换装后的效果。\n\n也可以使用英文提示词：Change the outfit with the uploaded one，生成结果贴合度极高，几乎挑不出毛病。\n\n除了 OOTD 和换衣，还有网友解锁了更多玩法，比如给设计线稿上色、让设计草图登上 T 台等。\n\nX 博主 @ZHO\\ZHO\\ZHO 丢给 Nano Banana 一个动漫角色，让其变成线稿手绘图，我们可以看到，生成的线稿图保留了角色的姿势和服装细节。\n\n再继续上传线稿图和色卡，输入提示词：准确使用色卡为图二人物上色。\n\nNano Banana 就能根据色卡方案，为角色的发色和服装配色进行全套替换。\n\n底下有网友用同样的方法尝试了另一种色卡，生成效果也很惊艳。\n\n网友 Yana Welinder 则用 Nano Banana 将一张服装设计草图，转换成时装秀成品。\n\n在过去，时尚创意的呈现往往需要冗长的流程与大量人力投入，而现在，AI 生成技术让这个周期被极大压缩。\n\n在不久的将来，像 Nano Banana 这样的工具，不仅能帮设计师更快试验灵感、让造型师提前预览效果，也能让普通人轻松尝试不同风格，找到属于自己的穿搭灵感。\n\n你还开发出哪些 Nano Banana 更多好玩的场景？评论区聊聊啊～\n\n参考链接：\n\nhttps://x.com/op7418/status/1961703552512118925\n\nhttps://x.com/ai\\for\\success/status/1962426574399320412\n\nhttps://x.com/ZHO\\ZHO\\ZHO/status/1960652077891510752\n\nhttps://x.com/yanatweets/status/1961451861934051726\n\n© THE END \n\n转载请联系本公众号获得授权\n\n投稿或寻求报道：liyazhou@jiqizhixin.com\n\n文章原文"},{"docId":"article_1f20089c-d525-40d8-8850-3c9bba21dd2b","title":"多模态大模型持续学习系列研究，综述+Benchmark+方法+Codebase一网打尽！","link":"http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650989423&idx=3&sn=1ae461e2d8f38fe4ca85de3721ab6529&chksm=854d7cd143680746a36f622bb7f403c5a06c0fc1c5200ef1c3dbe4bcd0ad1042ec8c19e77e37&scene=0#rd","pubDate":"2025.09.05 12:28:00","pubAuthor":"机器之心","content":"!Image\n\n本系列工作核心作者： 郭海洋（自动化所博士生）、 朱飞 （中科院香港院AI中心AP）、 曾繁虎 （自动化所硕士生）、 刘文卓 （自动化所博士生）、 赵宏博 （自动化所博士生）。通讯作者为自动化所博士生导师张煦尧研究员和刘成林研究员。团队长期从事人工智能研究，成果发表于 CVPR、ICCV、NeurIPS、ICLR、ACL、TPAMI、IJCV 等国际顶级会议与期刊。  \n\n近年来，生成式 AI 和多模态大模型在各领域取得了令人瞩目的进展。然而，在现实世界应用中，动态环境下的数据分布和任务需求不断变化，大模型如何在此背景下实现持续学习成为了重要挑战。为了应对这一问题，中国科学院自动化研究所联合中国科学院香港院 AI 中心系统性地研究了生成式 AI 和多模态大模型的持续学习，提出了一系列综述、方法、Benchmark 和 Codebase，为相关领域的研究者和实践者提供了全面支持。\n\n生成式 AI 的持续学习综述\n\n论文：Continual Learning for Generative AI: From LLMs to MLLMs and Beyond\n\n 论文链接：https://arxiv.org/pdf/2506.13045\n 项目主页：https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models\n\n研究动机：以大模型为代表的生成式 AI 模型的快速发展让现代智能系统具备了理解和生成复杂内容的能力，甚至在部分领域达到了接近人类的表现。然而，这些模型依旧面临着“灾难性遗忘”问题，即在学习新任务时，往往会导致已学任务性能的显著下降。为解决这一挑战，大量的研究提出了多种方法以增强生成式 AI 在实际应用中的适应性和扩展性。本文系统性地综述了生成式 AI 的持续学习方法，涵盖大语言模型（LLMs）、多模态大语言模型（MLLMs）、视觉语言动作模型（VLA）和扩散模型（Diffusion Models）。\n\n图 1：生成式 AI 中的持续学习示意图\n\n研究内容：本文围绕生成式 AI 的持续学习问题，系统性地综述了不同模型的训练目标、应用场景及技术方法。研究涵盖大语言模型在理解与生成中的知识保留与任务适应、多模态大模型处理跨模态数据时的抗遗忘能力、视觉语言动作模型在机器人动态环境中的行为迁移与适应，以及扩散模型针对个性化生成需求的增量学习。这些模型的持续学习方法主要包括架构扩展、正则化和回放策略，旨在平衡新任务学习与旧任务性能的保持。此外，研究还探讨了评估指标（整体性能、遗忘程度、泛化能力）和未来方向（高效机制、强化学习范式、多模态扩展等），为生成式 AI 的持续学习提供了全面参考。\n\n图 2：持续学习方法分类图\n\n多模态大模型持续学习：Benchmark 与方法\n\n传统的持续学习任务多聚焦于单模态场景，如图像或文本分类，但随着应用需求的复杂化，多模态任务逐渐成为核心。为此，我们提出了一系列新的 Benchmark 和方法，旨在推动多模态大模型持续学习的发展。\n\n论文 1：[ACL 2025] HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of Multimodal Large Language Model\n\n 论文链接：https://arxiv.org/pdf/2503.12941\n 代码链接：https://github.com/Ghy0501/HiDe-LLaVA\n 数据链接：https://huggingface.co/datasets/HaiyangGuo/UCIT\n\n研究动机：本文认为当前的多模态大模型的持续学习面临两大关键挑战：一是现有评测基准普遍存在与预训练数据重叠的问题，导致评估结果失真；二是传统方法难以平衡新任务学习与旧任务遗忘之间的矛盾。为此，本研究提出构建全新的 UCIT 评测基准，通过严格的 zero-shot 筛选机制，构建了全新的无信息泄露风险数据集。同时创新性地提出层次化解耦学习策略，旨在解决持续指令微调中的灾难性遗忘问题，为多模态大模型的持续学习提供可靠的评估框架和解决方案。\n\n研究方法：本文通过 CKA 相似性分析揭示了模型不同层级的任务特性差异：顶层具有任务特异性，其余层则保持任务通用性。基于此，本文提出的 HiDe-LLaVA 采用分层处理机制：在顶层引入多模态锚点驱动的动态专家选择模块，实现任务自适应；在其余层采用参数融合策略保留跨任务共享知识。实验结果表明，该方法可以有效缓解模型的灾难性遗忘现象，并且有效平衡了模型性能与计算资源效率。\n\n目前该研究已被 ACL 2025 主会接收，相关代码及数据已全部开源。\n\n图 3：HiDe-LLaVA 模型框架示意图。\n\n论文 2：[ICCV 2025] Federated Continual Instruction Tuning\n\n 论文链接：https://arxiv.org/pdf/2503.12897\n 代码链接：https://github.com/Ghy0501/FCIT\n 数据链接：https://huggingface.co/datasets/MLLM-CL/FCIT\n\n研究动机：当前多模态大模型的指令微调面临集中训练成本高、实用性差的问题，而现有联邦学习方案在动态环境中难以实现持续学习与知识保留的平衡。为此，本文首次提出 联邦连续指令微调（FCIT）基准，以解决分布式环境下持续学习的关键挑战。FCIT 基准包含同质（各客户端学习相同任务）和异质（各客户端学习不同任务）两种现实场景，通过 4 种实验设置和 12 个精选数据集，全面评估模型在非独立同分布数据和灾难性遗忘情况下的表现，为多模态大模型的联邦持续学习研究提供标准化评测框架。\n\n图 4：FCIT 设定示意图\n\n研究方法：为应对 FCIT 中的挑战，我们提出了 DISCO 框架，结合了 动态知识梳理（DKO） 和 子空间选择激活（SSA） 两种策略。DKO 利用全局服务器的动态缓存，存储并组织任务特定的参数，减少任务间和阶段间的冲突；SSA 则通过匹配输入特征与动态缓存中的任务子空间，选择性激活相关输出，同时过滤无关信息。实验结果表明，DISCO 在解决数据异质性和灾难性遗忘方面显著提升了模型性能，全面超越现有方法，并在 FCIT 基准上取得了最好的表现。\n\n目前该研究已被 ICCV 2025 接收，相关代码及数据已全部开源。\n\n论文 3：[EMNLP 2025] ModalPrompt: Dual-Modality Guided Prompt for Continual Learning of Large Multimodal Models\n\n 论文链接：https://arxiv.org/pdf/2410.05849\n 代码链接：https://github.com/AuroraZengfh/ModalPrompt\n\n研究动机：为缓解多模态大模型持续学习任务中的灾难性遗忘现象，本文认为现有解决方案存在显著局限性：基于数据回放的方法面临隐私泄露风险和存储成本压力，而模型扩展策略则不可避免地引发计算资源的线性增长。值得注意的是，当前研究尚未充分探索多模态数据在持续学习中的协同监督潜力。而当前领域内缺乏专门针对多模态特性的持续学习框架，因此需要开发一种既能利用图像-文本双重监督、又能避免计算膨胀的新方法，以实现高效且隐私安全的知识持续积累。\n\n图 5：ModalPrompt 模型框架示意图\n\n研究方法：本文提出 ModalPrompt 框架，利用多模态监督，通过构建任务特定的图像文本原型提示，结合双模态引导提示选择和多任务提示融合机制，实现了在无回放数据的情况下有效保留旧任务知识并提升新任务性能。此外，该方法通过动态提示选择降低计算复杂度，使推理速度提升 1.42 倍，同时显著减少存储和训练成本。\n\n目前该研究已被 EMNLP 2025 主会接收，相关代码已全部开源。\n\n论文 4. MLLM-CL: Continual Learning for Multimodal Large Language Models\n\n 论文链接：https://arxiv.org/pdf/2506.05453\n 代码链接：https://github.com/bjzhb666/MLLM-CL\n 数据链接：https://huggingface.co/datasets/Impression2805/MLLM-CL\n\n研究动机：本文认为现有的多模态大模型连续指令微调评测基准主要关注独立同分布（IID）场景下的领域知识评估，缺乏对模型基础能力（如 OCR、数学推理等）在非 IID 场景下的系统性评测。为此，本文提出了一个新的多模态大模型持续学习基准 MLLM-CL，涵盖领域持续学习（DCL） 和能力持续学习（ACL） 两种设置，分别针对同分布（IID）和非同分布（non-IID）场景下的领域知识和基础能力学习进行评估。\n\n图 6：MLLM-CL 基准示意图\n\n研究方法：为解决灾难性遗忘问题，本文提出了 MR-LoRA，通过领域或能力特定的 LoRA 模块实现参数隔离，避免任务间干扰，并设计了基于 MLLM 自身的多模态理解能力的路由选择器，仅需少量样本微调即可精准匹配输入与最优专家模块。实验表明，该方法在领域持续学习（DCL）和能力持续学习（ACL）任务上显著优于传统回放或模型扩展方法。\n\n论文 5. LLaVA-c: Continual Improved Visual Instruction Tuning\n\n 论文链接：https://arxiv.org/pdf/2506.08666\n\n研究动机：多模态大模型（如 LLaVA-1.5）在连续指令微调中面临的两大核心挑战：首先，传统的多任务联合训练存在任务平衡困难（需人工调整数据比例）和扩展成本高（新增任务需全量重训练）的固有缺陷；其次，现有持续学习方法虽能增量学习新任务，但普遍存在 \"基础模型退化\" 现象——模型过度拟合任务特定指令（如强制单字回答），丧失处理多样化指令的通用能力。\n\n图 7：LLaVA-c 模型框架示意图\n\n研究方法：本文提出了 LLaVA-c，通过两个核心技术改进 LLaVA-1.5 模型：1）谱感知巩固（SAC），基于奇异值分解的模型融合策略有效克服新旧知识冲突，相比传统的模型混合策略提升了任务兼容性；2）无监督查询正则化（UIR），通过约束未标注文本指令的特征空间偏移（L2 距离损失）防止基础模型退化，在零额外标注成本下保持指令跟随能力。本文在预训练和指令微调两阶段上都验证了所提出方法的有效性，在通用评价基准和下游任务指标上均取得了最优的性能，并且首次实现持续学习效果超越多任务联合训练。\n\n多模态大模型持续学习：代码仓库\n\n论文：MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark\n\n 论文链接：https://arxiv.org/pdf/2508.07307\n 代码仓库：https://github.com/Ghy0501/MCITlib\n\n研究动机：随着多模态大模型持续学习研究的蓬勃发展，各类创新方法和评估基准不断涌现，但研究社区始终缺乏一个系统化、标准化的开发与评测平台。为填补这一关键空白，我们推出了 MCITlib，一个开源的多模态持续指令微调代码仓库。MCITlib 集成了当前领域内 8 种主流算法，精心挑选了两个高质量基准（UCIT 和 DCL），有效避免信息泄露，为研究者提供了一个统一、公平的实验环境，便于全面评估不同方法的优劣。\n\n图 8：开源代码仓库 MCITlib\n\n未来，MCITlib 也将持续进行更新，扩展更多模型、任务和评测维度，为多模态大模型持续学习研究提供坚实助力。\n\n总结与展望\n\n赋予以多模态大模型为代表的生成式 AI 持续学习的能力是迈向人工智能通用化的重要一步。我们希望通过系统的综述、完善的 Benchmark、前沿的方法和开源的工具，能够为这一领域的研究者和应用开发者提供更多支持。未来，我们团队将继续深耕多模态大模型持续学习领域，探索更广泛的应用场景，持续推动该领域技术的发展与创新。\n\n© THE END \n\n转载请联系本公众号获得授权\n\n投稿或寻求报道：liyazhou@jiqizhixin.com\n\n文章原文"},{"docId":"article_c59f2145-0d78-4519-a68f-52bbcc1b0de9","title":"沉寂一个月，openPangu性能飙升8%！华为1B开源模型来了","link":"http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650989423&idx=2&sn=60103a153ef1d0c81dc744b2da8e68c3&chksm=85372112c76b8d119581f2ee57a434d8452755a5e4ac80de6e5f236660982494e682b5f91e7f&scene=0#rd","pubDate":"2025.09.05 12:28:00","pubAuthor":"机器之心","content":"机器之心发布\n\n机器之心编辑部\n\n在端侧 AI 这个热门赛道，华为盘古大模型扔下了一颗 “重磅炸弹” 。\n\n如今，云端大模型已经能侃侃而谈、答疑解惑。但如果这些 AI 大脑能被装进手机、摄像头甚至无人机，会带来怎样的变化？边缘设备上部署强大的 AI 模型已成为产业智能升级的关键路径。\n\n然而，端侧设备在算力、内存和功耗方面的严格限制，与传统超大模型的巨大计算需求形成了显著矛盾。现有方案往往陷入两难：要么采用性能羸弱的小模型，无法处理复杂任务；要么试图将云端大模型压缩后硬塞进端侧，结果精度严重下降或响应缓慢，难以满足实际应用需求。\n\n为了破解这一痛点，华为近日发布了专为昇腾端侧硬件打造的高性能语言模型 ——openPangu Embedded-1B。该模型虽然只有 10 亿参数，却通过软硬件协同设计显著降低推理延迟、提升资源利用率，并采用多阶段训练策略（包括从零预训练、课程学习式微调、离线 On-Policy 蒸馏、多源奖励强化学习）大幅增强各类任务表现。\n\n得益于多阶段训练与优化，openPangu Embedded-1B 在十亿参数的体量下实现了性能与效率的高度协同，成功将强大的大模型能力带到了端侧设备上，树立了「小模型大能力」的新标杆。\n\n评测成绩说明了一切，openPangu Embedded-1B 在多个权威基准上表现亮眼，创下了 10 亿参数级别模型的全新 SOTA 纪录。\n\n模型的整体平均分达到 63.90，不仅全面领先同类模型，甚至持平更大规模的 Qwen3-1.7B（63.69），充分体现了出色的参数效率。这表明，先进的训练与对齐方法可以比单纯扩大模型规模更具成效。\n\n在数学推理方面，openPangu Embedded-1B 经过强化学习对齐后取得了高分，其中在 GSM8K 数学基准上达到 82.76%，在 MATH 数学题集上达到 81.83%，均大幅领先同类模型。\n\n图：openPangu Embedded-1B 与其他模型在各项任务上的 0-shot 表现对比。可以看到，该模型在语言理解、数学、推理和编程等任务上均展现出明显优势，并显著缩小了与更大模型之间的差距。\n\n尤其值得关注的是，相比上个月开源的 openPangu Embedded-1B V1，V1.1 的平均分实现了 8% 以上的大幅跃升，这意味着开源盘古系列正在加速迭代升级。openPangu Embedded-1B 为资源受限的边缘设备带来了前所未有的智能水平，开辟了大模型端侧应用的新可能。\n\n 开源模型地址：https://gitcode.com/ascend-tribe/openPangu-Embedded-1B-v1.1\n 技术报告：https://gitcode.com/ascend-tribe/openPangu-Embedded-1B-v1.1/blob/main/docs/openPangu-Embedded-1B-report.pdf\n\n接下来，我们就一起揭晓这款模型背后的技术 “秘密”。\n\n软硬件协同设计：\n\n让 10 亿参数模型在端侧高效奔跑\n\nopenPangu Embedded-1B 是一款拥有 10 亿参数的自回归 Transformer 模型，专为昇腾 AI 处理器的端侧硬件平台优化设计。\n\n团队通过精心的软硬件协同，将模型架构与芯片特性深度结合：针对目标硬件的计算和内存特点，定制了合适的网络宽度和深度等超参数。换言之，模型的隐藏层规模、前馈网络维度等都与昇腾 Atlas 硬件的高效吞吐配置相匹配，确保每个计算单元都得到充分利用。\n\n在资源受限的设备上，这种 “软硬件协同” 的架构设计在模型深度和推理效率间找到了理想平衡点。\n\n图：在昇腾 Atlas 200I A2 硬件上，openPangu Embedded-1B 的推理延迟低于同级别的大模型。上表对比了不同 1B 量级模型的首 token 生成延迟（TTFT）和每 token 生成延迟（TPOT）。\n\n为了验证软硬件协同设计的效果，团队将 openPangu Embedded-1B 与其他相近规模模型进行了推理延迟基准测试。结果显示，在 Atlas 200I A2 硬件上，openPangu Embedded-1B 首字输出延迟仅约 1.8 秒，后续每词生成约 0.156 秒，而且 openPangu 精度相当。\n\n这一显著的速度优势充分证明了软硬件协同优化在端侧部署中的价值。\n\n两阶段课程学习：\n\n具备理性的快速响应\n\n为了让小模型也具备 “理性思维”，openPangu Embedded-1B 在微调阶段采用了课程学习式的 “两段式” 训练，模拟人类专家先深思后速答的学习路径。\n\n团队精心设计了难度递进的双阶段训练课程，循序渐进地塑造模型的推理能力：\n构建坚实的 “推理地基”：第一阶段，模型不追求响应速度，只专注于理性推理能力。它学习了海量包含详细推理过程的复杂问题解答示例，就像学生跟随导师一步步学习解题思路，理解背后的原理逻辑，打下扎实的逻辑推理基础。\n激发内化的 “快速直觉”：第二阶段，在模型具备强大的推理 “内核” 后，训练策略切换为提供大量简短的问答对，省略中间推理步骤。这好比学生掌握原理后开始练习快速作答，学会将深层思考内化于心，外化于行，以尽可能直接、迅速地得出答案。\n\n经过这两个阶段循序渐进的微调，模型深层次的推理能力被成功激活，openPangu Embedded-1B 在通用任务上的表现也全面提升。\n\n离线 On-Policy 知识蒸馏：\n\n师生协作的新范式\n\nopenPangu Embedded-1B 还进一步引入了一种 “学生主导，教师点拨” 的离线 On-Policy 知识蒸馏方法。不同于传统由教师单向灌输知识，这种方法更像智能辅导：先让 “小学生” 模型自主作答，再由 “大老师” 模型针对学生答案进行有的放矢的指导。\n\n蒸馏过程包括以下两个核心步骤：\n学生主导的自主探索：学生模型（1B）首先对训练问题自行生成答案，教师暂不介入，就像导师辅导前先让学生独立尝试解题，以了解其思路。\n教师约束下的精准点拨：随后更大的教师模型登场，但它并非直接给出正确答案，而是基于学生输出进行预测，在学生能力范围内提供针对性的提示，极大缩小了师生认知差距。\n\n通过这种离线 On-Policy 蒸馏，教师指导数据的生成与学生模型的训练实现了解耦，流程高度灵活；同时方法实现上改动极少（仅需增加一个蒸馏损失项），却令学生模型的准确率和泛化能力大幅提升。\n\n多源奖励强化学习：\n\n用反馈强化模型智慧\n\n在大规模 RL 训练阶段，团队开发了针对昇腾 NPU 集群的高效并行方案：通过容错同步调度和优先级数据队列最大限度利用上千加速卡资源，减少约 30% 的设备空闲；设计主机 - 设备权重共享和 NPU 端推理优化，使大规模强化学习在昇腾硬件上能够高效稳定运行。\n\n同时在算法上，团队对训练样本进行了难度筛选，过滤过易或过难的数据，引入 “零优势” 掩码忽略无效惩罚项，进一步保障了训练过程的稳定高效。\n\n为了指导模型自我提升，openPangu Embedded-1B 采用了多源奖励机制：针对数学、代码等可自动验证的任务使用基于规则的奖励，针对复杂开放任务则采用轻量级 LLM 模型来评估答案质量。\n\n奖励策略兼顾回答的正确性和格式规范，例如回答格式错误会受到严厉惩罚，答案错误但格式正确则扣减较小分值，而只有答案完全正确才能获得正奖励。这套精心设计的奖励信号确保模型在强化学习阶段获得全面而准确的反馈，不断优化自身能力。\n\n图：强化学习训练中，openPangu Embedded-1B 的平均奖励值和数学能力随训练迭代稳步提升。通过强化学习微调，模型的数学推理能力实现了飞跃式增强，而其他领域的性能也保持了稳定。\n\n展望：快慢思考融合的未来\n\n在极致挖掘小模型端侧潜能的同时，openPangu 研发团队也在探索让大模型的 “快思考” 和 “慢思考” 融为一体的新方向。目前，快慢思考模型往往面临两难：快速思考模式在复杂任务上力不从心，而慢思考模式应对简单问题又效率低下，难以兼顾速度与精度。\n\n对此，团队提出了一种自适应的快慢融合方案：在单一模型中同时提供快思考、慢思考和自动切换三种模式。模型可根据问题难度自动选择：简单问题快速作答，复杂问题深入推理后再作答，在保持接近慢思考模型精度的同时，大幅提高了易答问题的推理效率。\n\n据悉，openPangu-Embedded-7B 模型已应用自适应快慢融合策略，并在 7B 量级模型中取得了领先水平，其升级版本也将很快开源。\n\n可以预见，随着快思考 / 慢思考自适应融合等技术的引入，更大规模的端侧模型将同时实现高推理质量和高响应速度，为行业应用带来 “双优” 的 AI 能力。未来，随着端侧 AI 加速向实用化与普惠化迈进，算力受限设备也能享受云端级别的智能体验。\n\n© THE END \n\n转载请联系本公众号获得授权\n\n投稿或寻求报道：liyazhou@jiqizhixin.com\n\n文章原文"},{"docId":"article_e4215528-0cae-43d0-89ec-a6966178cfd8","title":"Nano Banana爆火之后，一个神秘的「胡萝卜」代码模型又上线了","link":"http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650989423&idx=1&sn=84fdd5fa90a0188815ac65bab1dd03d7&chksm=8542bdb1cb38eb3091cde815e59448c3d0acca479466beb2f217923aa76e0cc79eb5f41ae3f0&scene=0#rd","pubDate":"2025.09.05 12:28:00","pubAuthor":"机器之心","content":"机器之心报道\n\n机器之心编辑部\n\n以前，每当上线一个新模型，大家总要绞尽脑汁想个响亮又不撞车的名字。\n\n不得不说，有时候名字起得太出彩，甚至能把模型本身给卷下去。别人还没搞懂它能干嘛，名字已经在朋友圈刷屏了。\n\n不过现在，大家的创意又放飞起来了，尤其是那些还没正式上线的神秘模型。不管是动物，还是水果都能被拿来命名。刮起这波命名潮流的，毫无疑问是 OpenAI 率先下场的草莓 Strawberry，当时 Sam Altman 一张草莓图就引发了广大网友热议：\n\n在这之后，彷佛这种命名方式成为一种趋势，如 AI 初创公司 Recraft 神秘模型「red\\panda」（小熊猫），再到最近的谷歌「Nano Banana」。\n\n你别说，这些可可爱爱的名字真的把大模型带到一个新的高度。就拿火出圈的 Nano Banana 来说，公开后其实有正式的名字 Gemini 2.5 Flash，但大家更喜欢叫它 Nano Banana。\n\n就在刚刚，我们又发现了一个新的神秘模型：Carrot（胡萝卜），代码能力超级强。\n\n看来大家的起名方向开始转向蔬菜了。你别说，名字起得随意，模型能力可一点不差。\n\n这款神秘模型来自 Anycoder （一个专注于 AI 辅助编程的应用程序或平台）。打开 Anycoder 界面，拉到 model 那一栏，我们发现了胡萝卜，就像下面展示的。\n\n除了胡萝卜，我们还发现了 DeepSeek V3、Gemini 2.5 Pro、Grok-4、GPT-5 等众多明星模型：\n\n地址：https://huggingface.co/spaces/akhaliq/anycoder\n\n经广大网友验证，这款模型编程能力超级强：\n\n比如制作兔子躲避胡萝卜的游戏，并让胡萝卜作为子弹。看起来还挺好玩的，密密麻麻的胡萝卜追击着兔子，兔子巧妙地进行着躲避：\n\n来源：https://x.com/alfredplpl/status/1963755927229882462\n\n再来一个，生成体素宝塔花园：\n\n来源：https://x.com/\\akhaliq/status/1963783651021422907\n\n在 P5.js 中生成超粒子动画，是不是很炫酷。\n\n来源：https://x.com/ivanfioravanti/status/1963693728960295248\n\nX 知名博主 AK 用 transformer .js 制作了一个「gemma-3-270m」聊天机器人，运行效果非常好，能回答很多问题：\n\n来源：https://x.com/\\akhaliq/status/1963662702099701980\n\n看到这么炫酷的代码能力，大家的好奇心也是上来了。\n\n评论区都在猜测这是哪家的模型，有网友认为来自谷歌，毕竟他们刚发布了图像模型 Nano Banana，再来一个代码模型不是不可能。水果之后，总该轮到蔬菜了。\n\n还有网友猜测是不是月之暗面的 Kimi 。\n\n但我们觉得可能性为 0，因为就在刚刚，Kimi K2 0905 版本发布，具有更强的代码能力，要是 Kimi 的话，现在应该公开了。\n\n来源：https://mp.weixin.qq.com/s/Lac1gHCmuQ1mxTCWnSmuEA\n\n等等，不会是阿里的 Qwen3 系列吧？\n\n反正，大家也是各种猜测。所以，胡萝卜到底是谁家的？欢迎评论区留言。\n\n© THE END \n\n转载请联系本公众号获得授权\n\n投稿或寻求报道：liyazhou@jiqizhixin.com\n\n文章原文"},{"docId":"article_a35cf432-7748-4c5a-a603-96daf95eb7f0","title":"今天，特朗普闭门宴请了大半个硅谷的CEO，马斯克老黄没来","link":"http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652625324&idx=1&sn=1f21e457b495182aebd959d2ffcb52d2&chksm=f0a7cb45ef06edc5009ca8d63ef4396599db5416dd7713afea06b7e2b5a70b95d86fd537d9fc&scene=0#rd","pubDate":"2025.09.05 19:09:00","pubAuthor":"新智元","content":"新智元报道 \n\n编辑：艾伦\n\n【新智元导读】今天，特朗普在白宫宴请了大半个科技圈的CEO共商国事。马斯克和老黄没有到。他们到底聊了些什么呢？\n\n今日，特朗普于白宫特设闭门晚宴，招待了全球影响力巨大的大半个科技圈的巨头领袖们。\n\n在有限的视野里，我们可以看到不少熟悉的面孔，但也有一些只能看到背影或干脆在镜头外。\n\n还好，白宫记者Jennifer Jacobs在X上公布了这份晚宴的完整名单。\n\n上下滑动查看  \n\n出席名单：\n\n Tim Cook\n Mark Zuckerberg\n Bill Gates\n Sergey Brin\n Gerelyn Gilbert-Soto\n Sam Altman\n Greg Brockman\n Anna Brockman\n Safra Catz\n Gal Tirosh\n Jason Chang\n Meredith O’Rourke\n Nathalie Dompé\n Tony Fabrizio\n Dylan Field\n John Hering\n Jared Isaacman\n Sunny Madra\n Satya Nadella\n Chamath Palihapitiya\n Sundar Pichai\n Mark Pincus\n Vivek Ranadivé\n David Sacks\n Shyam Sankar\n Jamie Siminoff\n Lisa Su\n Alexandr Wang\n Sanjay Mehrotra\n David Limp\n Susie Wiles\n\n他们都是谁？\n\n这份完整出席名单中，有很多我们很熟悉的科技巨头领袖，也有一些我们还不太熟悉的人。\n\n下面简单介绍一下名单中出现的人。\n\n可以粗略地将出席者分成六类。\n\n一、科技公司领袖\n\n 蒂姆·库克（Tim Cook）：苹果公司CEO；\n\n 马克·扎克伯格（Mark Zuckerberg）：Meta创始人兼CEO；\n\n 比尔·盖茨（Bill Gates）：微软联合创始人及慈善家；\n\n 谢尔盖·布林（Sergey Brin）：谷歌联合创始人；\n\n 萨提亚·纳德拉（Satya Nadella）：微软CEO；\n\n 桑达尔·皮查伊（Sundar Pichai）：谷歌及Alphabet公司CEO；\n\n 苏姿丰（Lisa Su）：AMD CEO；\n\n 桑杰·梅赫罗特拉（Sanjay Mehrotra）：美光科技CEO；\n\n 大卫·林普（David Limp）：前亚马逊设备业务负责人，现任蓝色起源CEO；\n\n 萨芙拉·卡兹（Safra Catz）：甲骨文公司CEO；\n\n二、 AI 与创业公司领袖\n\n 萨姆·奥特曼（Sam Altman）：OpenAI CEO，前Y Combinator总裁；\n\n 格雷格·布罗克曼（Greg Brockman）：OpenAI联合创始人兼总裁；\n\n 亚历山大·王（Alexandr Wang）：Scale AI创始人兼CEO；\n\n 迪伦·菲尔德（Dylan Field）：Figma联合创始人兼CEO；\n\n 约翰·赫林（John Hering）：Lookout安全公司联合创始人；\n\n 桑尼·马德拉（Sunny Madra）：创业者与投资人，曾任福特X副总裁；\n\n三、投资人\n\n 查马斯·帕里哈皮蒂亚（Chamath Palihapitiya）：Social Capital创始人，硅谷投资人，All-In播客主持人；\n\n 娜塔莉·东佩（Nathalie Dompé）：查马斯·帕里哈皮蒂亚（Chamath Palihapitiya）的妻子，意大利制药公司继承人、风险投资人；\n\n 维韦克·拉纳迪维（Vivek Ranadivé）：科技企业家，萨克拉门托国王队老板；\n\n 大卫·萨克斯（David Sacks）：「PayPal Mafia」成员之一，Craft Ventures合伙人，总统科学技术顾问委员会主席，被称为「白宫AI和加密货币沙皇」；\n\n 沙亚姆·桑卡尔（Shyam Sankar）：Palantir首席运营官；\n\n 马克·平库斯（Mark Pincus）：Zynga创始人；\n\n四、政客\n\n 梅瑞迪斯·奥罗克（Meredith O’Rourke）：美国政治筹款专家，曾参与特朗普竞选团队；\n\n 托尼·法布里齐奥（Tony Fabrizio）：共和党民调专家，特朗普顾问；\n\n 苏西·怀尔斯（Susie Wiles）：白宫办公厅主任，特朗普高级竞选顾问；\n\n五、其他商业人物\n\n 加尔·蒂罗什（Gal Tirosh）：甲骨文公司CEO萨芙拉·卡兹（Safra Catz）的丈夫，以色列科技与媒体企业家；\n\n 贾里德·艾萨克曼（Jared Isaacman）：亿万富翁企业家，Inspiration4太空任务指挥官；\n\n 杰米·西米诺夫（Jamie Siminoff）：Ring智能门铃创始人；\n\n 安娜·布罗克曼（Anna Brockman）：格雷格·布罗克曼（Greg Brockman）的妻子；\n\n六、未知\n\n我们尚未能通过公开信息查实这些人的身份。\n\n 杰森·张（Jason Chang）；\n 杰雷琳·吉尔伯特-索托（Gerelyn Gilbert-Soto）。\n\nWhere is Elon Musk?\n\n看过名单，我们发现少了两位科技圈核心老朋友。\n\n其中之一便是马斯克。\n\n自从DOGE解散、马斯克与特朗普公开决裂后，二人的关系变得格外尴尬。\n\n面对网友的质疑，马斯克只是简单回应：「他们邀请我了，但我没空。我派代表去了。」\n\n网友们也并不买账，纷纷问马斯克，所谓「代表」究竟是谁：\n\n我们也确实未能在出席名单中找到公开身份明显和马斯克有关联的人物。\n\n另一位未出席的重磅人物，则是英伟达的老黄。\n\n英伟达（NVIDIA）创始人兼CEO黄仁勋\n\n据华尔街日报推测，老黄或许只是不喜欢这种只方便说场面话的大场面，更喜欢通过一对一单独会面来高效地沟通可落实的解决方案。\n\nhttps://www.wsj.com/politics/trump-tech-ceo-rose-garden-dinner-1fee2de3\n\n这场晚宴，在聊些什么\n\n当天晚宴早些时候，美国第一夫人梅拉尼娅·特朗普（Melania Trump）主持了白宫AI教育特别工作组圆桌会议，奥特曼和谷歌CEO桑达尔·皮查伊（Sundar Pichai）等人参加了。\n\n原本这场晚宴应在户外露台举行，由于突然下雨，大家转移到餐厅内进行。\n\n相对完整的晚宴视频在此：\n\n特朗普在第一任期内与深蓝的加州的科技领袖们的关系并不算融洽。\n\n在本次任期中，特朗普一转常态，对AI和加密货币鼎立支持，也想藉此晚宴拉近同硅谷的关系。\n\n特朗普夸赞大家：\n\n这张桌子聚集了最聪明的人，这绝对是一群高智商的人。\n\n特朗普也就研发AGI所需的耗电巨大的数据中心的供电问题进行了表态，会努力帮助大家扫清障碍。\n\n特朗普还点了一下谷歌，提到了谷歌搜索反垄断案，谷歌取得了胜利，并将这份诉讼起源归咎于拜登。\n\n除了谷歌，特朗普也没忘了Meta，他向小扎询问英国对Meta的言论打压问题。\n\n小扎被突如其来的问题吓了一跳，连忙说他没有听。\n\n特朗普打趣说：\n\n这是你的政治生涯的开始。\n\n总统发话了，桌上众人纷纷开始表达决心：\n\nMeta和苹果分别承诺了在美国投资6000亿美元；\n\n库克说：\n\n我要感谢你们奠定了基调，让我们能够在美国进行重大投资，并在这里发展一些关键的、先进的制造业。我认为这充分体现了你们的专注力、领导力以及对创新的关注。\n\n盖茨说：\n\n感谢总统奠定了基调，让我们能够在美国进行重大投资，并在这里发展一些关键的制造业和先进制造业。感谢总统在白宫晚宴上表现出的令人难以置信的领导力。餐桌上的人们正在改变世界。\n\n微软CEO纳德拉说：\n\n我认为，你们所做的一切，包括搭建一个平台，让世界其他国家不仅可以使用我们的技术，而且比任何其他替代方案都更加信任我们的技术，或许才是最重要的。而你们，你们以及你们的政策，真的帮了大忙，非常感谢。\n\n奥特曼说：\n\n感谢总统成为如此支持商业、支持创新的总统。这是一个非常令人耳目一新的变化……我认为这将使我们长期引领世界，如果没有你的领导，这一切都不会发生。\n\n「白宫AI和加密货币皇帝」大卫·萨克斯打圆场：\n\n大家都是为了美国。\n\n参考资料：  \n\nhttps://www.wsj.com/politics/trump-tech-ceo-rose-garden-dinner-1fee2de3  \n\nhttps://www.businessinsider.com/whos-who-tech-leaders-attended-dinner-white-house-altman-pichai-2025-9  \n\nhttps://www.usatoday.com/story/news/politics/2025/09/04/trump-hosts-white-house-dinner-without-musk/85973868007/  \n\nhttps://www.axios.com/2025/09/05/trump-tech-dinner-ceo-zuckerberg-musk\n\n文章原文"},{"docId":"article_f6d64a1a-7a17-456a-813e-045fb9bfb772","title":"0.01%参数定生死！苹果揭秘LLM「超级权重」，删掉就会胡说八道","link":"http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652625324&idx=2&sn=9d2f51848fe7b7d3d2cce4e3831df390&chksm=f0851981b667439db5f98181ade116267f5e6524067c692e855e8126a18a4fc916c4ce69d6f2&scene=0#rd","pubDate":"2025.09.05 19:09:00","pubAuthor":"新智元","content":"新智元报道 \n\n编辑：元宇\n\n【新智元导读】苹果研究人员发现，在大模型中，极少量的参数，即便只有0.01%，仍可能包含数十万权重，他们将这一发现称为「超级权重」。超级权重点透了大模型「命门」，使大模型走出「炼丹玄学」。\n\n0.01%参数定生死！\n\n在删掉极少量参数后，大模型立刻变得胡言乱语起来，在零样本任务中只会瞎猜，原来的那股聪明劲儿全没了。\n\n但是，如果保留这些极少量参数，即使删掉成千上万其他参数，大模型的智力依然在线，几乎看不出有什么影响。\n\n如果拿一棵树比喻，剪掉树（大模型）的几千片叶子（冗余参数）不会伤筋动骨，但只要砍掉树干上的一个关键节点（核心参数），整棵树可能就死掉了。\n\n这个核心参数，就是大模型中存在的极少数关键性/高敏感度参数。\n\n有时甚至只需一个，就能对大模型的整体功能产生巨大影响。\n\n论文地址：https://arxiv.org/abs/2411.07191\n\n近日，苹果研究人员在论文《大语言模型中的超级权重》（The Super Weight in Large Language Models）中，将上述现象，称为「超级权重现象」。\n\n如上图1左侧显示，带有超级权重的原始Llama-7B，能顺利接着生成合乎逻辑的内容。\n\n而在图1右侧，当超级权重参数被剪枝后，Llama-7B就开始胡言乱语，生成的全是毫无意义的文本。\n\n这生动诠释了什么叫「打蛇打七寸」：\n\n剪枝一个「超级权重」的特殊参数，就可以完全破坏大模型的能力。\n\n让大模型「科学瘦身」\n\n「超级权重」的发现，为大模型在端侧部署，扫清了道路。\n\n在实际应用中，大模型庞大的体格（动辄数十亿甚至数千亿参数），想要部署在移动端等一些低预算、资源受限等环境中，就像把大象塞进冰箱，往往会面临巨大挑战。\n\n如果只是简单粗暴的等比压缩或简化，就好比削足适履，只会导致模型质量显著下降。\n\n更为合理的做法，是让大模型「科学瘦身」，比如缩小模型的规模和计算复杂度，从而降低内存与功耗。\n\n这时，超级权重就显得至关重要。\n\n在模型压缩和简化过程中，要避免碰到这些数量虽小，却牵一发而动全身的「命门级」参数，避免它们被显著修改（通过压缩）或被完全移除（剪枝）。\n\n即使它们的比例可以小到0.01%，但对于拥有数十亿参数的模型，仍然意味着有数十万个单独权重。\n\n苹果研究人员发现，如果动了它们，就可能破坏LLM生成连贯文本的能力，比如让困惑度上升3个数量级，这样大模型就几乎「读不懂」语言了。\n\n又或者使大模型的零样本学习准确率降低到「瞎猜」的水平，这意味着大模型的智能也几乎废掉了。\n\n如何定位「超级权重」？\n\n许多研究都显示出：少量最大幅值的异常值对模型质量至关重要。\n\n对于拥有数十亿参数的模型，极少量的参数，即便是只有0.01%，仍可能包含数十万权重。苹果研究人员将称这个单标量权重为超级权重（super weight）。\n\n超级权重，会放大某个特征，产生超级激活。\n\n超级权重，会诱发相应稀有且幅度巨大的激活离群值，研究人员将之称为super activations（超级激活）。\n\n所谓激活，是指模型在前向传播时，每一层神经元的输出值。\n\n它们通常是在超级权重之后出现，并在随后的层中以一种恒定的幅度和位置持续存在，而不受输入提示词的影响。\n\n比如，一旦某个超级权重参与计算，它会把输入信号放大成异常大的数值，于是紧接着的层中就出现超级激活。\n\n并且，超级激活与超级权重所在通道一致。\n\n于是，研究人员就提出了一种高效定位超权重的方法：\n\n通过超级激活来定位超级权重：利用检测向下投影输入和输出分布跨层中的尖峰来定位超级权重。\n\n为了促进公开研究，研究人员还将一部分常见、公开可用的LLM超级权重标记了出来，如下表2：\n\n研究人员发现，大多数模型每个张量中的超级权重不超过三个。\n\n即使超级权重数量最多的模型（例如Phi-3-mini-4k-instruct）也只包含六个。\n\n研究人员还通过图2，展示了超级权重触发超级激活，以及超级激活的传播机制。\n\n图2-I中蓝紫色方框中展示了超级权重的触发，它通常出现在较早层的down projection（降维投影）。\n\n这好比在一开始就有一个「功放器」，把某个信号突然放大到极高的音量。\n\n图2-Ⅱ中表示超级激活通过跳跃连接传播，用蓝紫色线表示，它表示激活不是一次性消失，而是层层跳跃传播下去。\n\n这好比扩音器的噪音通过音响的电路一路传到所有扬声器，无论后续放什么音乐，那个噪音始终存在。\n\n图2-Ⅲ中表示，在最终的输出logits（预测分布）里，超级激活会产生压制停用词（stopwords）的效果。\n\n而移除超级权重，会导致停用词可能性增加，用蓝紫色堆叠条表示。\n\n在图3中，down\\proj输入在层2中，仅有一个大幅度的激活值（super activation），这是超级激活首次出现的地方。\n\n图4表示，一旦在第2层被触发，超级激活会在随后的所有层中以相同的幅度、相同的位置持续存在，而不受输入的影响。如果把超级权重剪掉，超级激活的强度会下降75%。\n\n图5中显示了超级权重对停用词的抑制作用。\n\n研究人员发现，移除超级权重会导致停用词概率增加2-5倍，这在各种LLMs中都存在。\n\n同时，非停用词的概率急剧下降，减少2-3倍，低至0.1%的概率。\n\n整体上看，超权重会影响输出Token的概率分布。\n\n从图6可以看出，增强超权重，可以在一定程度上提高模型准确率。\n\n超级离群值\n\n模型量化的「关键钥匙」\n\n量化是压缩模型、降低模型内存需求的一种强有力技术。\n\n其中影响量化质量的，是一种重要的指标离群值（outliers）。研究人员将超级权重和超级激活统称为超级离群值。\n\n超级离群值，为人们认识大模型，改进大模型压缩技术，提供了一把重要的钥匙。\n\n在该项研究中，研究人员考虑的是一种最简单的量化形式——即非对称的就近取重量化（asymmetric round-to-nearest quantization）：\n\n保留超级权重参数，是大模型「瘦身」的一个黄金原则。\n\n研究人员发现，只要以高精度保留超级激活，通过简单的就近取整（round-to-nearest）量化，也能将模型质量提升到与当前最先进方法相当的水平。\n\n如表3所示，在与FP16、Naive W8A8、SmoothQuant三种模型量化方法的比较中，就近取整量化虽然效果略次于SmoothQuant，但优于Naive W8A8，尤其是在不需要校准数据的前提下，实用性更强。\n\n同样，如果在保留超权重的同时，对其他权重异常值进行裁剪，就近取整量化，也可以实现更好的压缩比。\n\n这意味着只需处理少量「超级离群值」，就能显著提升压缩质量。\n\n研究人员认为，与需要处理数十万离群权重的方法相比，这无疑是一种更友好的硬件方案。\n\n它可以在提升模型效率的同时，又能尽可能保留原有性能。\n\n这也使得强大的LLM应用，在资源受限的硬件上部署和高质量运行，成为可能。\n\n激活量化与权重量化\n\n为了全面展示超级权重的影响，研究人员将研究范围扩大到更多大模型：OLMo（1B和7B版本）、Mistral-7B以及Llama-2-7B。\n\n表4显示，处理超级激活可以提升激活量化效果。\n\n研究人员遵循SmoothQuant的设置，用FP16算术模拟W8A8量化。\n\n研究结果凸显了超级激活，在量化期间维持模型性能的关键重要性。\n\n研究人员对Llama-7B的分析显示，AWQ将超级权重放大了12倍，这印证了他们对超级权重重要性的判断。\n\n如图7，蓝线RTN显示，如果不处理超级权重，随着量化块变大，模型性能急剧下降；紫线Ours表示，如果恢复超级权重，模型准确率下降更平缓，即使大块量化也能维持较好性能。\n\n这说明，只要针对单个超级权重进行特殊处理，就能显著提高量化的稳定性和可扩展性。\n\n探索超级离群值的版图\n\n苹果研究人员的发现，为未来研究打开了多条道路。\n\n毫无疑问，进一步探索超级权重与超级激活的起源及其精确机制，将对LLM的运行动态，带来更深入的洞见。\n\n同样的，理解这些超级权重参数，如何在训练过程中获得如此「超级」的影响力，也可以为未来的模型设计、训练策略提供更有针对性的指导。\n\n从另一个角度看，在更广泛的模型架构和训练范式中，展开对超级权重的研究，也有助于揭示它们的角色和形成机制。\n\n这些都将帮助我们解锁，构建更高效、更稳健、更可解释大模型的创新方法，让大模型告别「炼丹玄学」。\n\n作者简介\n\nMengxia Yu\n\nMengxia Yu是圣母大学计算机专业博士生，此前在北京大学获得计算语言学学士学位，本论文是她在苹果公司实习期间完成的。\n\n参考资料：  \n\nhttps://machinelearning.apple.com/research/the-super-weight\n\n文章原文"},{"docId":"article_0d661d80-4d1c-4e99-8624-c71e7d882c1e","title":"长视频生成可以回头看了！牛津提出「记忆增稳」，速度提升12倍","link":"http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652625182&idx=3&sn=f62c21d8ed5768b3d88122c46354a634&chksm=f07d69f250996d5ed7d8b971a110959f040cd09b262764f8f73f72b38fda310f620cc441a70a&scene=0#rd","pubDate":"2025.09.05 13:35:00","pubAuthor":"新智元","content":"新智元报道 \n\n编辑：LRST\n\n【新智元导读】VMem用基于3D几何的记忆索引替代「只看最近几帧」的短窗上下文：检索到的参考视角刚好看过你现在要渲染的表面区域；让模型在小上下文里也能保持长时一致性；实测4.2s/帧，比常规21帧上下文的管线快\\~12倍。\n\n当你用一张图「逛」一套房子，来回转场、回到起点，还希望厨房看起来还是原来的厨房——这件事对视频生成模型并不容易。\n\n牛津大学团队提出VMem（Surfel-Indexed View Memory）：把「看过什么」写进一种叫surfel的几何小片里，下一次生成时只取真正相关的过往视角当上下文，实现了「一致性更强、资源更省、速度更快」的效果。\n\n论文链接：https://arxiv.org/abs/2506.18903\n\n· 几何做「记忆目录」\n\n把过去生成的视图按3D表面元素（surfel）索引；每个surfel记录「哪几帧见过我」。\n\n新视角来时，渲染surfel看谁「出现频率最高」，直接取这些帧当参考。显式遮挡建模，检索更靠谱。\n\n· 小上下文，大一致性\n\n在RealEstate10K、Tanks and Temples等基准上，尤其是团队提出的回环轨迹（cycle-trajectory）评测里，VMem在长序列回访同一位置时显著更稳。\n\n· 即插即用\n\n记忆模块可挂在SEVA等图像集生成骨干上；把上下文从K=17减到K=4仍能守住指标，还把时延砍到4.2s/帧（RTX 4090）。\n\n为什么「回头看」这么难？\n\n两类主流路线各有痛点：\n\n 重建+外延补洞（out-painting）：先估几何再补图，误差会叠罗汉，越走越歪；\n 多视图/视频式条件生成：不做几何，但要吃很多参考帧，算力开销大、上下文窗口短，走远就忘。\n\nVMem重新审视第二类：与其看「最近」，不如看「最相关」。相关性的度量来自几何可见性。\n\n写入（Write）：新生成的帧用CUT3R等点图预测得到稀疏点云→转成surfel（位置、法向、半径）→把「看到我的帧编号」写进surfel的索引集合；相近surfel做合并；整体放入八叉树方便检索。\n\n读取（Read）：面对一组待生成相机位姿，先求一个平均相机，从该视角渲染surfel属性图，统计每个像素投票到的「出现过的帧编号」，挑Top-K最高频作为参考视图集合；对位姿相近的参考做NMS去冗余。\n\n生成（Gen）：把（Top-K参考图像+参考/目标相机的Plücker表达）喂给图像集生成器（论文默认SEVA），一次自回归生成M帧。\n\n直观理解，surfel是「看过的表面贴纸」，上面写着「是谁看过我」；新相机来时， 把贴纸从新角度投影，谁名字出现最多就把谁叫来帮忙。\n\n向世界模型的可插拔记忆层\n\n为什么世界模型需要这样的记忆？\n\n世界模型通常靠隐式隐状态（latent state / RNN / Transformer cache）来跨时保留信息，但在长视野、部分可观测（POMDP）的场景中，隐式状态容易「遗忘」早期细节、且不可解释。\n\nVMem提供的是显式、可查询、几何对齐的外部记忆：以surfel*为「记忆索引」，把「谁看过我」这类可见性线索结构化存起来。这样做带来三点直接收益：\n\n 长时一致性：记忆容量与步数解耦；跨数百步仍能稳定回访同一地点与外观。\n 可解释与可裁剪：按可见性投票做检索，遮挡/误配更少；内存可按区域/密度/热度做剪枝。\n 高效取证：把「看很多不相干的历史帧」变成「只看与当前表面相关的少量关键帧」，大幅缩小上下文与算力。\n如何接入现有世界模型？（三种常见用法）\n\n外部记忆（External Memory）：把VMem作为Key-Value存储，Key=surfel（位置/法向/半径等），Value=出现过该surfel的帧与特征。模型在每步预测前，通过相机姿态渲染surfel可见性图，检索Top-K参考视图与特征并融合到当前状态更新。\n\n检索 前端 （Retrieval Front-End）：在视频/多视图生成backbone（如图像集扩散或时空Transformer）前，用VMem先选参考视图，再走主干网络；等价于把「上下文选择」外包给几何索引。\n\n策略+世界模型联合（RL/Embodied）：将VMem作为共享记忆*供「世界模型+策略」共同读写：世界模型用它做长期一致的模拟，策略用它做定位/导航/回忆证据，减少长时信用分配的难度。\n\n实验与结果\n\n评测设置：单图起步，沿真值相机轨迹自回归生成；长期评估看≥200帧位置；团队额外提出回环轨迹，专测「走一圈再走回去」的一致性。\n标准长期设置*\n\nVMem在多数指标优于公开基线；当轨迹很少回访时，优势不完全体现在LPIPS/PSNR上，但肉眼一致性更好。\n回环轨迹*\n\nVMem在PSNR、LPIPS等指标上对比LookOut、GenWarp、MotionCtrl、ViewCrafter等普遍领先，回到起点时的外观与布局更一致。\n\n效率：LoRA微调的K=4/M=4版本+VMem，\\~12×推理提速（4.2s/帧 vs 50s/帧），而画质和相机对齐指标接近或更优于K=17的大上下文。\n\n消融：把检索策略换成「最近帧/相机距离/FOV重叠」，一致性明显变差；说明基于surfel的可见性投票是关键。K越小越显著。\n和谁不一样？\n\n相比重建+补洞线：VMem不把几何当最终表征，只用它做检索，因此对几何误差相对更鲁棒；\n\n相比FOV/距离/时序检索：VMem的surfel显式考虑遮挡与可见区域的真实重叠，相关性更准；\n\n相比隐藏状态记忆（如世界模型的隐表征）：VMem的「记忆」是可解释的空间索引*，便于裁剪与加速。\n\n局限与展望\n\n非实时：扩散采样仍需多步；作者估计未来可借助单步图像集模型与更强算力进一步加速；\n\n数据域：微调主要在RealEstate10K（室内），对自然景观/动态物体的泛化仍待拓展；\n\n评测标准：现有指标对「真正的多视角一致性」刻画有限，回环协议是一个开端，还需更系统的评测。\n\n参考资料：  \n\nhttps://arxiv.org/abs/2506.18903\n\n文章原文"},{"docId":"article_c0753ff2-fa35-4db8-a13a-9c4c40c62b8e","title":"新天终启，万象智生！从AlphaGo到GPT-5，新智元十年见证ASI创世纪","link":"http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652625182&idx=4&sn=003e18e9ca15f54b0dfc027f67a84681&chksm=f0d15e4d29d2b0b79c0b943a026a53f23cd30a652aecf8a211f0617d13a860e3f0e16807c109&scene=0#rd","pubDate":"2025.09.05 13:35:00","pubAuthor":"新智元","content":"新智元报道 \n\n编辑：编辑部\n\n【新智元导读】在AI以指数级加速迈向ASI的2025年，新智元迎来十周年历史时刻，将于9月7日在北京中关村软件园举办盛大峰会。大会以「新天终启，万象智生」为主题，汇聚百度王海峰、英伟达赖俊杰、百川智能王小川、昆仑万维方汉、未来智能胡郁等多位重量级嘉宾，共同探讨芯片、大模型、Agent、具身智能及医疗智能等全球ASI最前沿突破，展望ASI重塑人类社会的恢弘篇章。\n\n2025年，我们站在新纪元、新天地的起点。\n\n在新旧世代的边缘，我们有幸见证一场AI智能的「宇宙大爆炸」！\n\n这场史无前例的AI革命是由人类亲手点燃的一束智慧火花，短短几年间，席卷全球。\n\n对人类文明的颠覆与再造，其深刻影响远胜于170万年前人类开始使用火。\n\nAI基座大模型能够独立完成任务的时间如同「奇点爆炸」般指数级增长。\n\n2025年，大模型的发布节奏和速度越来越密集，越来越快。两到三个月，就会有一个性能超越所有对手的全新大模型横空出世刷新榜单。\n\nOpenAI在过去半年里，连续发布了o3-mini、o3、o4‑mini、o3‑pro、GPT‑4.5 、GPT‑4.1、\n\n以及开源模型gpt‑oss‑120b和gpt‑oss‑20b。\n\nGPT-5的发布更是引发全球对于「智能温情」、「AI伴侣」的大讨论，无数人高喊还我「GPT-4o」，但手里依然还是放不下GPT-5。\n\n谷歌在5月年度I/O大会上正式发布Gemini 2.5系列。\n\n8月5日，DeepMind发布Genie 3世界模型，引领我们初窥缔造虚拟世界的旖旎风光。\n\nAI的发展甚至不再依赖人类的指引，开始自我迭代、自我进化。\n\n谷歌DeepMind发布的AlphaEvolve就是一个由Gemini驱动的编码智能体，可以通过「自行进化」来设计高级算法。\n\n回到半年前，2025年初，几乎没有人想到，DeepSeek会一举引爆开源全球浪潮。\n\n半年过去，中国造的各种大模型已经在世界开源领域独领风骚，通义千问Qwen 2.5‑Max、Qwen3、Qwen3‑Coder、智谱GLM-4.5、还有Kimi-K2等占据所有榜单。\n\n就在刚刚，8月26日《国务院关于深入实施人工智能+行动的意见》正式发布！其中重点提到：\n\n 2027年发展目标：新一代智能终端、智能体等应用普及率超过70%。\n 2030年发展目标：新一代智能终端、智能体等应用普及率超过90%。\n 2035年发展目标：我国全面步入智能经济和智能社会发展新阶段，为基本实现社会主义现代化提供有力支撑。\n\n未来十年，在中国开源模型的探索，人工智能+行动的不断推进下，一个全面智能+的社会的必将到来！\n\n大模型外，AI智能体、具身智能也在井喷式发展，成为世界经济增长的「新极点」。\n\n百度秒哒、昆仑万维的天工超级智能体、百川的医疗大模型、科大讯飞星火、智源生命大模型与具身智能大脑等百家争鸣。\n\n划时代的变革吹响号角，新的智能世界即将开启！\n\nASI爆点降临之际，2025年9月7日，也迎来新智元成立十周年！\n\n新智元将在9月7日重磅发布《2025 新智元ASI前沿趋势报告——新天终启 万象智生》。\n\n2027年人工智能将达到ASI的临界点！\n\n新天终启，新的智能世界即将开启，人类将抵达ASI的临界点。\n\n随着全球资源的天文数量级涌入，大模型智能将持续全方位跃迁！\n\n人类的大脑和20-30万年前的智人祖先并无根本差别，但短短几年间，AI的智商已经开始远超人类。博士级的AI已达到数十亿量级的全球用户规模。\n\n目前智商最高的GPT-5 Pro已经超过120\n\n万象智生，未来十年，宇宙万物与人类文明都将在更新的超智能天地中获得无限生命。\n\n未来，具身智能、智能体和世界模型将迎来全面大爆发！\n\n回顾新智元过去的十年，也是AI向ASI急速进化的十年。\n\n但是这十年的发展并不总是一帆风顺，伴随着低落和阵痛。\n\n自2017年Transformer架构提出以来，AI技术并没有马上进入爆发式增长，缺少产品形式与市场爆发点。\n\n直到2022年末，ChatGPT上线5天内用户突破100万，才全面激发社会关注与产业爆发。\n\n回想十年前，谷歌的AlphaGo击败欧洲冠军职业围棋棋手樊麾两段，让人类初窥机器智能的力量。\n\n十年前，谷歌刚刚发布开源深度学习框架TensorFlow，深度学习开始成为AI的「代名词」。\n\n也就是十年前，2015年12月，埃隆·马斯克、山姆·奥特曼等人共同成立了非营利组织OpenAI。\n\n十年后，OpenAI已经变成估值最高的AI独角兽，而马斯克似乎也放下了自己的「火星梦」，全力以赴为xAI摇旗呐喊。\n\n拉长时间线，人工智能（Artifical Intelligence）自1956年达特茅斯会议诞生以来，历经多次寒冬，才终于走到了今天ASI爆发的前夜。\n\n十年，沧海桑田。\n\n回首十年前，2015年9月7日，新智元成立。\n\n人工智能领域的先行者就坚信，人类是智慧代码的载体，而机器智能将突破人类局限，孕育超级智能。\n\n正如新智元创始人杨静女士在新智元创刊词中发出的誓言：\n\n在智能面前，万物应该平等；在宇宙当中，唯有智能永生。\n\n人机融合，令生命与文明长存。万物智能，创造美好新智界。\n\n我们坚信，未来的魅力在于未知，世界的未来在于先知。\n\n公元2015年9月7日，新智元以对ASI的矢志信仰为引擎，启动了穿越智能宇宙的航程。\n\n十年星舰穿越，我们感恩每一位生态伙伴的并肩奋战，共同绘制ASI新天地的壮丽图景。\n\n2025年，岁序流转，镌于尘篇。\n\n十年来，新智元每天与百万读者共同经历的故事，写就ASI最难忘的启示录。\n\n让我们相约9月7日，新智元十周年峰会的精彩开启，见证ASI全新历史时刻的到来！\n\n演讲嘉宾介绍\n\n杨静 新智元创始人兼CEO\n\n杨静，2015年9月创办新智元，2016年3月出版专著《新智元 机器+人类=超智能时代》，2016年10月联合主办世界人工智能大会，并出版《中国人工智能产业发展报告》。2017年至2019年举办AI World世界人工智能峰会，主办新智元产业技术峰会。2019年9月出版专著《智周万物：人工智能改变中国》，该著作已译为12个语种在全球印行。2020年至今，主办数场重磅AI家论坛、元宇宙新人类论坛、大模型论坛，成立业界顶级社群。新智元成立十年，新智元微信公众号用户破百万，全矩阵平台流量连年过亿，已成为中国人工智能领域极具影响力的三位一体产业链生态平台。\n\n王海峰 百度首席技术官、深度学习技术及应用国家工程研究中心主任\n\n王海峰，百度首席技术官、深度学习技术及应用国家工程研究中心主任。被授予「国家卓越工程师」称号。 自然语言处理领域最重要的国际学术组织ACL首位华人主席、ACL亚太分会创始主席、ACL Fellow、IEEE Fellow、国际欧亚科学院院士。兼任中国工程师联合体、中国电子学会、中国中文信息学会副理事长。 以第一完成人获国家技术发明二等奖、国家科技进步二等奖、中国专利金奖、北京市科技进步一等奖、吴文俊人工智能科技进步特等奖、中国电子学会科技进步一等奖等。获光华工程科技奖、首届全国创新争先奖、首个吴文俊人工智能杰出贡献奖。入选国家百千万人才工程，被授予「有突出贡献中青年专家」称号。享受国务院政府特殊津贴。入选北京学者。\n\n赖俊杰 NVIDIA工程和解决方案副总裁\n\n赖俊杰博士，于清华大学电子工程系取得本科及硕士学位，于法国INRIA获得博士学位。赖俊杰博士研究重点是GPU架构和性能分析模型，在高性能计算、并行计算、人工智能及其应用、性能分析和优化方面拥有丰富的经验。现任NVIDIA工程和解决方案副总裁，赖俊杰团队支持中国主要客户的技术需求，并开发行业解决方案和定制软件产品。\n\n方汉 昆仑万维董事长兼CEO\n\n方汉，毕业于中国科学技术大学近代物理系，曾任职于中国科学院高能物理研究所、拓林思、亚信以及千橡互动，2008年3月协助周亚辉先生创立昆仑万维。方汉先生拥有30年互联网从业经验，从1994年开始参与和倡导开源运动，是中文Linux奠基人、中文Linux四剑客之一，著作国内第一本Linux书籍《Linux实用大全》；也是国内最早的网络安全专家，曾负责三大电信运营商的全网安全审计，负责研发了国内市场占有率最高的网页游戏《三国风云》，为中国互联网最资深的参与者。\n\n王小川 百川智能创始人兼CEO\n\n王小川博士，百川智能创始人兼CEO。高中时期用吴文俊消元法，首次在微型机下完成初等几何命题的全部证明。1996年获得国际信息学奥林匹克竞赛金牌。担任搜狗CEO期间，主持开发了搜狗输入法、浏览器、搜索等国民级产品。曾获中国青年五四奖章、中国青年科技奖、中国电子学会科学技术奖科技进步奖一等奖、北京市科学技术奖一等奖等荣誉。2023年创立百川智能，认为医疗是通用人工智能最重要的应用领域之一。以为生命建模型，为人类造医生为使命，以「造医生、改路径、促医学」为理念助力生命科学和大众健康事业发展。作为国内唯一一家专注医疗领域的大模型创业企业，百川智能在AI医疗领域取得了一系列突破和进展：2024年10月，百川医疗增强大模型在中国执业医师考试和美国执业医师考试中的准确率超越OpenAI；2025年2月，基于Baichuan-M1打造的全球首个AI儿科医生正式在北京儿童医院的多学科专家会诊中心「上岗」，获13位专家一致认可。\n\n林咏华 智源研究院副院长兼总工程师\n\n林咏华，现任北京智源人工智能研究院副院长兼总工程师, 主管大模型研究中心、人工智能系统及基础软件研究、产业生态合作等重要方向。IEEE女工程师亚太区领导组成员，IEEE女工程师协会北京分会的创始人。曾任IBM中国研究院院长，同时也是IBM全球杰出工程师，在IBM内部引领全球人工智能系统的创新。从事近20年的系统架构、云计算、AI系统、 计算机视觉等领域的研究。林咏华有超过50个全球专利，并多次获得ACM/IEEE最佳论文奖。获评2019年福布斯中国50位科技领导女性。\n\n尤洋 潞晨科技创始人兼董事长\n\n尤洋，潞晨科技创始人兼董事长，清华大学硕士，加州伯克利大学博士，师从该校电子与计算机学院院长JamesDemmel院士，现为新加坡国立大学计算机系的校长青年教授(PresidentialYoungProfessor)。他是世界上唯一一位35岁以下，在4个顶会（AAAI,ACL,IPDPS,ICPP）上以领导者的身份（第一作者或通讯作者）获得最佳论文/杰出论文的人。他曾创造ImageNet、BERT、AlphaFold、ViT训练速度的世界纪录，并被ScienceDaily，TheNextWeb，i-programmer等几十家媒体广泛报道，相关技术被广泛应用于谷歌，微软，英特尔，英伟达等科技巨头。他近三年以第一作者身份在NIPS，ICLR，Supercomputing，IPDPS，ICS等国际重要会议或期刊上发表论文十余篇，总计发表论文近两百篇。他曾获福布斯30岁以下精英榜(亚洲)、IEEE-CS超算杰出新人奖、胡润U35创业先锋、福布斯中国·全球最具影响力华人精英Top100、中国经济十大领军人物、中国智能计算创新人物、财富·中国40位40岁以下的商界精英等奖项。他曾任职于谷歌，微软，英伟达，英特尔，IBM等国际知名厂商。\n\n刘嘉 清华大学基础科学讲席教授\n\n刘嘉，清华大学基础科学讲席教授，清华大学心理与认知科学系主任、人工智能学院教授（兼）、北京智源人工智能研究院首席科学家。长江学者特聘教授，国家杰出青年基金获得者，「万人计划」科技创新领军人才，教育部自然科学奖一等奖获得者。\n\n张铭 北京大学计算机学院二级教授\n\n张铭，北京大学计算机学院二级教授，博士生导师，北大本硕博，教育部计算机课程教指委委员，2021中国计算机教育学会CCF杰出教育奖获得者。研究方向为大语言模型、图机器学习、科学智能等。谷歌学术被引22900余次，H因子57。获得机器学习顶级会议ICML 2014唯一的最佳论文奖、自然语言处理顶会ACL 2025最佳论文奖，合作提出的图嵌入模型LINE和原生稀疏注意力模型NSA受到广泛关注。带领北大《数据结构与算法》团队，两门课程获评首批国家级一流本科课程（排名第一位），从2008年起开设《科技创新实践》等双创课程，在基础学科能力和创业实践方面给学生全方位的培养。\n\n新智元十年回顾圆桌论坛嘉宾介绍\n\n主持人：\n\n杨静 新智元创始人兼CEO\n\n杨静，2015年9月创办新智元，2016年3月出版专著《新智元 机器+人类=超智能时代》，2016年10月联合主办世界人工智能大会，并出版《中国人工智能产业发展报告》。2017年至2019年举办AI World世界人工智能峰会，主办新智元产业技术峰会。2019年9月出版专著《智周万物：人工智能改变中国》，该著作已译为12个语种在全球印行。2020年至今，主办数场重磅AI家论坛、元宇宙新人类论坛、大模型论坛，成立业界顶级社群。新智元成立十年，新智元微信公众号用户破百万，全矩阵平台流量连年过亿，已成为中国人工智能领域极具影响力的三位一体产业链生态平台。\n\n圆桌嘉宾：\n\n张代君 中国三星首席副总裁\n\n张代君，中国三星首席副总裁，曾任三星电子中国研究院院长11年(2013-2024)，期间主导三星电子在华AI,5G/6G先行研究开发和商业化落地。\n\n苏奎峰 腾讯自动驾驶、车载地图总经理\n\n苏奎峰，清华大学计算机科学与技术博士，腾讯自动驾驶/车载地图总经理。多年从事自动驾驶、无人平台和相关应用技术研究工作，主要研究领域包括人工智能、多源信息融合、自动驾驶、智能交通、数字孪生、具身智能等。现任中国自动化学会智能自动化专业委员会委员，智能网联汽车标准委员会委员。曾获得国家科技进步二等奖1项，军队科技进步一等奖1项，军队科技进步2/3等奖多项，出版学术专著8部，自动驾驶及国防领域专利20+个，发表专业论文40余篇。\n\n吴甘沙 驭势科技董事长、CEO\n\n吴甘沙，驭势科技董事长、CEO。前英特尔中国研究院院长、首席工程师，领导英特尔大数据长期技术战略规划，成立英特尔唯一的机器人实验室。2016年创业成立驭势科技。发表29篇学术论文，拥有28项美国专利、10余项PCT、数十项中国专利。担任中国自动化学会常务理事，电动车百人会、中国电子学会、中国人工智能学会的理事，中国计算机学会杰出会员，中国通信学会车联网专委会委员，ACM SIGAI中国分会副主席，复旦大学校友会北京分会副会长等。\n\n李立军 慈星股份董事、副总裁\n\n李立军，博士、教授级高工，慈星股份董事、副总裁，中国（浙江）机器人及智能装备创新中心董事/投融资委员会主席，中国毛纺织协会“十三五”毛纺织行业“科技带头人”。毕业于东南大学、澳大利亚墨尔本大学，中国自动化学会高级会员，IEEE会员，中国纺织工程学会纺织工业人工智能专委会副主任委员，科技部国家“智能机器人”重大专项评审专家、教育部/科技部/工信部国家重大人才工程评审专家、浙江省智能制造专家委员会特聘专家、浙江省首批“万人计划”科技创新领军人才、宁波市有突出贡献专家。曾作为项目主要完成人获国家科技进步二等奖。\n\n黄华 北京师范大学人工智能学院教授、院长\n\n黄华，北京师范大学人工智能学院教授，院长，智能技术与教育应用教育部工程研究中心主任。国家杰出青年基金和中国青年科技奖获得者，国家万人计划科技领军人才。主要从事图像/视频处理、人工智能方面的研究工作，部分成果在工业、国防、互联网行业得到应用。兼任中国计算机学会常务理事、教育工作委员会副主任；中国图像图形学会常务理事、多媒体专业委员会主任；中国自动化学会常务理事。\n\n李建忠 奇点智能研究院院长、CSDN高级副总裁  \n\n李建忠，于2016年发起创办全球机器学习技术大会（ML-Summit），是人工智能领域极具影响力的高端技术研讨与交流平台。对人工智能、产品创新、软件架构有丰富经验和深入研究。近年来提出科技创新的“范式转换立方体 ParaShift Cube”，相关研究和引起业界强烈关注。他对软件开发、架构设计深有研究，于2005年发起创办C++ 及系统软件技术大会，是ISO-C++国际标准委员会委员（机器学习组）。他创立了奇点智能研究院，主要研究以⼤模型为主的⼈⼯智能范式，为包括世界五百强在内的多家企业提供人工智能与创新战略咨询。\n\n李立武 芯矩开物（NeuMatrix）创始人兼CEO\n\n李立武，AI算力芯片公司——芯矩开物（NeuMatrix）创始人和CEO。李立武拥有25年高端芯片设计和管理经验，设计过多款当时世界最先进 GPU / CPU / FPGA及AI芯片产品系列。曾在英伟达、英特尔，Sun Microsystems, RapidSilicon等公司担任设计副总裁，高级总监等职位。李立武在多国从零组建过十支以上芯片设计团队。负责从预算、资源、设计、流片、封装、验证每一环节，设计和领导经验涵盖芯片研发所有方面。\n\n十年求索，奇点共盟。\n\n今天，我们已站在地球AI进化的奇点边缘。\n\n大模型掀起智能革命的惊天海啸，重新编程人类社会的底层架构，颠覆万年文明史既有的路线图。\n\n人类正迈向一个由超级智能定义的新宇宙，ASI的耀眼光束点亮了新天地之永恒旅程。\n\n在此万年一遇，智能大爆发的新纪元起点，新智元集结ASI先驱者，一起吹响超智能时代的号角！\n\n新智元于2025年9月7日在北京・中关村软件园・国际会议中心举办十周年峰会，主题为「新天终启，万象智生」——ASI 纪元恢弘开启，宇宙万物与人类文明都将在更新的智能天地中获得无限生命！新智元十周年峰会既是ASI世代的新技术坐标，也是新智元AI家十年重聚的欢庆节日，让ASI先驱者共启星际新程。\n\n9月7日，我们相聚于毗邻新智元AI家的「大飞碟」旁，共同见证「新天终启，万象智生」的激荡时刻！\n\n感谢新智元的读者这十年来对我们的不离不弃。\n\n扫描二维码，直达峰会预约链接。\n\n新智元十周年峰会媒体支持，同步直播，期待您的到来，关注我们，一起迎接新智元！\n\n文章原文"},{"docId":"article_ca1666cb-c9af-4e57-87df-cecfc4876f39","title":"力压哈佛MIT！北交大、清华勇夺2025国际大学生程序设计竞赛金牌","link":"http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652625182&idx=1&sn=93bd32e16ab70e8709023755f303ec87&chksm=f06fdae5b3b825e4b55d189f7b37d05113aa1aed7b6b4c8f11689a69aa8db6eb879b17ec1813&scene=0#rd","pubDate":"2025.09.05 13:35:00","pubAuthor":"新智元","content":"新智元报道 \n\n编辑：KingHZ 艾伦\n\n【新智元导读】北京交通大学力压哈佛MIT，和清华一同拿下ICPC全球总决赛金牌！前10中，中国队更是包揽了4席。\n\n北京交通大学力压哈佛MIT，携手清华勇夺ICPC全球总决赛金牌！\n\n昨晚，全球顶尖大学齐聚阿塞拜疆首都巴库，揭晓了第49届ICPC世界总决赛获奖名单。\n\n北京交通大学、清华大学、北京大学压美国哈佛大学、MIT等一众世界顶尖计算机强校，取得了全球排名Top 5！\n\n北京交通大学解出12道难题中的10道，一举刷新该校ICPC历史最好纪录，登顶中国第一，世界第三，豪取金牌！\n\n上一届他们排名世界第七夺得银牌，这次终于拒绝遗憾。\n\n北京交通大学参赛队\n\n清华大学解出了12道难题中的9道，也夺得了金牌。\n\n清华大学参赛队\n\n而北大也同样解决了12道题中的9道，但因罚时（每次提交答案若不正确会+20分钟罚时）获得银牌。\n\n北京大学参赛队\n\n中国科学技术大学也同样解决了12道题中的9道，也因罚时以5分钟之差惜败MIT，获得铜牌。\n\n中国科学技术大学参赛队\n\n冠亚军则分别是俄罗斯圣彼得堡国立大学和日本东京大学。\n\n此外，在决赛开幕式的表彰环节，ICPC Asia EC委员会成员、北京大学计算机学院罗国杰副教授荣获「全球杰出训练营组织奖」（Outstanding Community Engagement Award）。\n\n传送门：https://cs.pku.edu.cn/info/1263/3944.htm\n\n金牌得主\n北交大金牌选手*\n\n北京交通大学「羊羊羊」（miemiemie）代表队的郭昱哲、夏鹤鸣、斯修远三位同学，本次成功勇夺金牌，创造了北京交通大学ICPC竞赛的最好成绩！\n\n上一届ICPC全球总决赛，北京交通大学的这支队伍获得了银牌。\n\nmiemiemie队（中文队名：羊羊羊）：左起分别为参赛队员郭昱哲、斯修远、夏鹤鸣，许华婷（教练）\n\n下面让我们简单介绍一下三位队员。\n\n夏鹤鸣同学是北京交通大学詹天佑学院计算机科学与技术专业2021级本科生。\n\n进入大学之前，夏鹤鸣似乎并没有刻意投入大量时间在OI竞赛上。（我们于OIerDb未查到获奖信息）\n\n就连他的Codeforces（最流行的算法竞赛平台）账号，也是他进入大学后才注册的。\n\n然而，他在大学里展现出了对编程和算法的惊人天赋。\n\n从大一升大二的暑假，2022年7月10日第一次参加打Codeforces比赛开始，他的编程水平急速攀升。\n\n仅仅两个月后，2022年9月30日，他的Codeforces Rating就来到了2200分。\n\n后续，他步步为营，最高分已经冲至3220分，达到了国际金牌水平！\n\n而这一切成就，都离不开勤奋。他已经刷了超过4000道烧脑的高难度算法题。\n\n郭昱哲同学毕业于河北衡水一中，目前是北京交通大学计算机学院人工智能专业2022级本科生。\n\n他在高中阶段是一名OIer，曾拿过NOI银牌。\n\n目前，他的Codeforces Rating为2434分。\n\n斯修远同学毕业于浙江省诸暨中学，目前是北京交通大学数学与统计学院信息与计算科学专业2023级本科生。\n\n斯修远同学在高中也是一名OIer，曾获得NOIP省一等奖、APIO铜牌、信息学冬令营金牌等荣誉。\n\n目前，他的Codeforces Rating为2208分。\n\n清华金牌选手*\n\n清华大学更是屡次获得ICPC全球总决赛金牌。\n\n2007年，楼天城带领清华大学队在ACM/ICPC全球总决赛中获得第二名。\n\n楼天城是中国公认的大学生计算机编程第一人，经常以一人单挑一个队，在CEOI、ACM界无人不晓其大名，人称「楼教主」\n\n本次清华大学再次不负众望斩获金牌！\n\n这支获奖队伍的队员分别是：万成章、张景行、刘一平。\n\n万成章同学是华东师范大学第二附属中学2023届毕业生。\n\n他在高中阶段拿到过NOI金牌和APIO金牌，保送就读清华计算机专业。\n\n目前他的Codeforces Rating为2552分。\n\n张景行同学是北京大学附属中学的一名优秀的毕业生。\n\n在高中阶段，他也曾获得NOI全国金牌。\n\n目前他的Codeforces Rating已高达3047分。\n\n刘一平同学是山东省潍坊第一中学毕业生。\n\n高中阶段，他于2023年勇夺IOI世界金牌，保送清华。\n\n目前，他的Codeforces Rating也是3000分左右。\n\n真题一览\n\n本次决赛，共有12道题目。\n\n完整题库见：https://worldfinals.icpc.global/problems/2025/\n\n其中问题C，全军覆没：\n\n北京交通大学，获得亚洲东部区域冠军，首先解决问题L：沿着阳光行走问题。\n\n首先解决问题队伍，还会获得对应形状的气球。\n\n哈佛大学，获得北美区域冠军，首先解决K题：宝藏地图。\n\n亚洲西部冠军沙里夫技术大学Sharif University of Technology，首先解决问题A。\n\n圣彼得堡国立大学率先解决了问题B。\n\n芝加哥大学率先解决了问题 J。\n\n卡尔斯鲁厄理工学院Karlsruhe Institute of Technology率先解决了问题I。\n\n伊利诺伊大学厄巴纳-香槟分校率先解决了问题F。\n\n北京大学率先解决了问题D。\n\nICPC：计算机软件领域的奥林匹克\n\n国际大学生程序设计竞赛（International Collegiate Programming Contest，简称ICPC）是世界上规模最大、水平最高的国际大学生程序设计竞赛之一。\n\n赛事由各大洲区域赛（Regional Contests）和全球总决赛（World Finals）两个主要阶段组成，每个赛季持续时间数月，来自全球6大洲、超过100个国家和地区的数千所高校的近五万名大学生参与该项赛事。\n\n经过五十余年的发展，国际大学生程序设计竞赛已经成为全球最具影响力的大学生计算机竞赛，被誉为计算机软件领域的奥林匹克。\n\n1977年，在ACM计算机科学会议期间举办了首次总决赛，并演变成为目前的一年一届、多国参与的国际性比赛。\n\n这次在阿塞拜疆首都巴库举办，由ADA大学承办。\n\n之前，上海交通大学、哈尔滨工程大学、北京大学先后承办过国际大学生程序设计竞赛全球总决赛。\n\n中国大陆高校自1996年开始参加此项赛事的亚洲区预赛。\n\n1996年起设立中国大陆地区预选赛赛区，当年由上海大学承办。之后在大陆地区设置多个赛点，由各大学轮流主办地区性竞赛。\n\nICPC以团队的形式代表各学校参赛，每队由3名队员组成。\n\n比赛期间，每队使用1台电脑需要在5个小时内使用C、C++、Python、Java或Kotlin中的一种编写程序，解决8到13个问题。\n\n程序完成之后提交裁判运行，运行的结果会判定为一种结果并及时通知参赛队：\n\nAC（答案正确）、WA（答案错误）、TLE（超出时间限制）、MLE（超出内存限制）、RE（运行错误）、PE（格式错误）。\n\n每队在正确完成一题后，组织者将在其位置上升起一只代表该题颜色的气球。\n\n这次决赛中，北京大学率先解决了问题D并获得了对应的气球\n\n最后的获胜者为正确解答题目最多且总用时最少的队伍。\n\n每道试题用时将从竞赛开始到试题解答被判定为正确为止，其间每一次提交运行结果被判错误的话将被加罚20分钟时间，未正确解答的试题不记时。\n\n让我们恭喜这些优秀的中国学子！\n\n参考资料：\n\nhttps://x.com/ICPCNews/status/1963641952974344273\n\nhttps://worldfinals.icpc.global/scoreboard/2025/index.html\n\nhttps://www.youtube.com/watch?v=93DYRPNRrRA&t=8s\n\nhttps://icpc.pku.edu.cn/jj/index.htm\n\n文章原文"},{"docId":"article_a77d4f89-758a-435a-9276-2c753255050f","title":"视频理解新标杆，快手多模态推理模型开源：128k上下文+0.1秒级视频定位+跨模态推理","link":"http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247823406&idx=1&sn=eca439fa7d33b95b4931323754a75ff5&chksm=e91bda8d47c32b004d42b7834c2aa36ef718ff2021adfe0d7b585234f2cf408624a19bf57ffb&scene=0#rd","pubDate":"2025.09.05 18:56:00","pubAuthor":"量子位","content":"克雷西 发自 凹非寺  \n量子位 | 公众号 QbitAI\n\n能看懂视频并进行跨模态推理的大模型Keye-VL 1.5，快手开源了。\n\n相比此前的预览版本，Keye-VL 1.5的时序定位能力进一步升级，并且支持跨模态推理。\n\n还创新性地提出Slow-Fast双路编码机制，给模型带来了128k的超长上下文窗口，而且速度与细节兼顾。\n\n成绩上，不仅在Video-MME短视频基准斩获73.0的高分，还在通用、视频和推理场景的大量Benchmark当中领跑同级模型。\n\n视频元素0.1秒级定位，还支持推理\n\n要说Keye-VL-1.5最大的亮点，研究团队认为主要有三个，也就是开头提到的128k上下文、突出的短视频理解能力、 以及更强的Reasoning能力。\n\n在视频理解场景当中，这三项能力能够同时得以展现。\n\n首先是时序信息理解，Keye-VL-1.5能够准确判断特定物品在视频中出现的时间点，而且精确到0.1秒级。\n\n比如在这段26秒带货视频片段中，介绍了一款充电宝，其中一个环节是将其装进包包，以体现便于携带。\n\nKeye-VL-1.5看完这段视频后，准确回答出了其中包包出现的时间——22.3-23.8秒。\n\n而其他模型或者时间只精确到秒而且还不准确，或者干脆不说时间，直接数起了镜头。\n\n再来是描述能力，Keye-VL-1.5能够详细描述视频画面场景和细节。\n\n例如对于上面这段视频，Keye-VL-1.5给出了这样的描述：\n\n并且Keye-VL-1.5还具备视频推理能力，能够根据前序视频内容推断后续事件发生原因。\n\n在这段宠物视频当中，大狗做出了一个咬小狗耳朵的动作，而Keye-VL-1.5要分析大狗为什么要咬。\n\n其实答案在视频当中已经以文字的形式写了出来，但是Keye-VL-1.5的解释更加详细，并进一步用视频中的后续变化来加强自己的观点。\n\n跑分方面，Keye-VL-1.5在多项公开Benchmark以及内部评测中都拿到了同尺寸模型中的最高分。\n\n在MMBench、OpenCompass等综合类基准中，Keye-VL-1.5的成绩均超越Qwen2.5-VL 7B，并取得多个SOTA。\n\n在图像推理强相关的AI2D、OCRBench等数据集中，也均超出同级其他模型。\n\n针对视频理解，Keye在Video-MME、TempCompass和LongVideoBench上，成绩同样领先于Qwen2.5-VL 7B等模型。\n\n包含视觉的数学与逻辑推理维度上，Keye也保持了领先优势。\n\n除了这些公开数据集，Keye团队还构建了200条面向短视频应用的内部多维度评测。\n\nKeye-VL-1.5-8B在人类标注的五项指标（正确性、完整性、相关性、流畅度、创造性）上获得3.53分的综合成绩，较预览版本提升了0.51分，也超过了作为对比的竞品模型。\n\n那么，Keye-VL是如何实现的呢？\n\n视频理解，也用上了快慢思考\n\n模型架构和快慢编码策略\n\n整体设计上，Keye-VL-1.5采用了“视觉Transformer（ViT）+MLP投影器+语言解码器”的三段式架构。\n\nViT将输入图像或视频帧切成14×14的patch序列，用自注意力捕捉全局空间关系。\n\n该ViT在初始化时直接继承SigLIP-400M参数，SigLIP是一种图文对比预训练方法，可让视觉特征天然带有语义对齐能力。\n\n为了在不裁剪的情况下处理任意分辨率，团队对ViT添加了“原生分辨率”支持，操作上先把绝对位置向量插值到任意尺寸，再引入2DRoPE（二维旋转位置编码）增强高分辨率外推。\n\nViT输出的patch特征经由两层MLP投影器送入语言解码器，语言端采用Qwen3-8B，并在其位置编码中加入3DRoPE。\n\n3DRoPE是在传统二维旋转角的基础上再增加一维“时间/深度”角度，目的是让文本token与视觉token按统一时序排序。\n\n针对视频的高帧率与高分辨率矛盾，Keye-VL-1.5还创新性地提出Slow-Fast编码策略。\n\n模型首先会对连续帧做patch级余弦相似度计算，若与最近一次“慢帧”（又称变化帧，低帧数高分辨率）相似度>95%则判定为“快帧” （又称静止帧，高帧数低分辨率），否则标记为新“慢帧”。\n\n处理时，慢帧保留高分辨率，快帧分配慢帧30%的token预算，再结合二分搜索，能够让总预算精确落在限制内，并在序列里插入时间戳特殊符号以标注帧界。\n\n通过这种视频快慢编码策略，Keye实现了性能与计算成本的有效平衡。\n\n四阶段渐进式预训练\n\n预训练采取四阶段渐进流水线，按照“先单模后多模、先对齐后扩窗”的顺序展开：\n\n Stage0，视觉编码器预训练：仅用SigLIP对比损失继续训练ViT，强化视觉语义，适应内部数据分布；\n Stage1，跨模态对齐：冻结ViT与Qwen，只训练MLP投影器进行大规模跨模态对齐；\n Stage2，多任务预训练：解冻全网络，在8K上下文下端到端优化，增强模型的基础视觉理解能力；\n Stage3，退火训练： 在精选高质量数据上进行微调，引入长上下文模态数据，把上下文拉长到128K。\n\n整个预训练语料超过1万亿token，数据源既包含LAION、DataComp、CC12M等公开多语言图文库，也有大规模自建图像、视频与文本。\n\n四阶段结束后，Keye团队对不同数据配比训练的“同质”权重与针对OCR、数学等薄弱项单独强化得到的“异质”权重进行模型融合，以减小偏差并提升鲁棒性。\n\n“同质模型”指的是在退火期采用相同网络结构和相似任务目标，但调整数据配比、样本难度或随机种子训练出的多份主干权重，这些模型彼此性能分布接近；\n\n“异质模型”则是利用与主干不同的专用数据域进行进一步精调而生成的专家权重，例如团队针对车牌、票据和街景文字额外收集/合成数据训练出的OCR-Expert。\n\n由于双方架构一致，融合过程可以通过直接权重插值实现，不引入推理时额外开销，却能将专家的局部能力注入通用模型。\n\nPost-training\n\nKeye-VL-1.5的训练后处理包含四个主要阶段：\n\n 第一步用监督微调结合多偏好优化（MPO）建立输出质量基线；\n 第二步通过五步流水线的大规模链式思考数据冷启动，为模型提供可靠的推理示范；\n 第三步在可验证奖励框架下采用GSPO算法并配合渐进提示采样做多轮强化学习，系统化提升通用推理能力；\n 最后一步以规则-生成式-模型三源奖励完成对齐强化学习，重点加强指令遵循、格式一致性与用户偏好一致性。\n\n在监督微调阶段，团队先构建包含750万多模态问答的候选池，用TaskGalaxy将样本映射到七万种任务标签，再刻意提高高难度类型的占比。\n\n随后进入MPO，以25万开源、15万纯文本和2.6万人工样本为基底，利用Keye-Reward模型分数和人工评估构造高低质配对，通过偏好损失函数让模型在同一问题上倾向得分更高的答案，从而进一步提升回答质量。\n\n有了质量可控的答案后，模型借助链式思考冷启动流水线迅速补齐推理深度，先自动生成带步骤的解答，再由第二模型逐步打分进行分级，中档样本经人工精修后复审，高分样本直接入库，为后续强化学习提供可靠冷启动权重。\n\n接下来进入通用强化学习，系统首先按照样本难度分组，然后利用GSPO在组内基于序列重要性权重裁剪优势函数，缓解长序列梯度不稳。\n\n当推理能力趋于收敛后，训练转入最后的对齐阶段。\n\n规则奖励通过正则和AST解析强制检查JSON、Markdown等结构与内容安全，生成式奖励由外部大模型评估逻辑一致性与语言风格，模型奖励则来自Keye-Reward模型的细粒度偏好分。\n\n三类信号动态加权，使最终模型既能遵循指令又能保持格式正确并符合用户偏好，同时有效降低无依据生成风险。\n\n团队成果多次亮相顶会\n\n说到快手大模型，我们可能更熟悉视频生成模型可灵，但实际上，快手在其他类型的大模型上同样有很强的实力。\n\n打造Keye-VL的Keye团队，是快手内部专注多模态大语言模型研发的核心AI部门，主攻视频理解、视觉感知与推理等前沿方向。\n\nKeye团队认为，整合视觉、语言和行为等多源数据的智能体，对于解锁更深层次的认知和决策至关重要。\n\n目前，Keye团队已经拥有大量成果，在今年的多个顶会上密集发布。\n\nICML 2025上，Keye团队提出了多模态RLHF框架MM-RLHF（2502.10391），通过120k人类偏好对比与批评式奖励模型，显著提升MLLM安全性及对齐性能。\n\nKDD 2025上，视觉语言模型治理框架VLM as Policy（2504.14904）获得了最佳论文提名。\n\n该框架通过VLM驱动内容质量与风险判定，显著提高短视频审核效率与准确率。\n\nCVPR 2025上，Keye团队也发布了两项成果。\n\n交错图文多模态数据集CoMM（2406.10462），提供了高一致性图文叙事样本，从而增强模型图文穿插理解与生成能力。\n\n视觉token压缩加速算法LibraMerging，采用位置驱动合并，在无需再训练的情况下大幅降低推理开销。\n\n还有ICLR 2025中，Keye有三项研究成果亮相，包括一种优化算法和两个数据集。\n\nMoE模型优化算法STGC（2406.19905），可以检测token梯度冲突并进行重路由，提升专家利用率与收敛速度。\n\n视频对话理解基准SVBench（2502.10810），构建了时序多轮问答链，评测LVLM在流式长视频场景的推理水平。\n\n还有视觉任务指令数据集TaskGalaxy（2502.09925），可以自动生成万级层级任务与40万余样本，增强模型跨任务泛化能力。\n\n在快手内部，Keye团队的这一系列成果，正在为短视频内容审核、智能剪辑、搜索与互动推荐等业务场景提供底层AI能力。\n\nKeye正在把多模态技术从实验环境推向千万级日常场景，验证复杂视频理解在真实业务中可行且高效，为同类技术的工程化落地提供了直接样本。\n\n技术报告： \nhttps://arxiv.org/pdf/2509.01563  \n代码： \nhttps://github.com/Kwai-Keye/Keye/blob/main/Kwai\\Keye\\v1\\5.pdf  \n模型权重： \nhttps://huggingface.co/Kwai-Keye/Keye-VL-1.5-8B  \n在线DEMO： \nhttps://huggingface.co/spaces/Kwai-Keye/Keye-VL-1\\5-8B\n\n一键三连 「点赞」「转发」「小心心」\n\n欢迎在评论区留下你的想法！\n\n— 完 —\n\n🌟 点亮星标 🌟\n\n科技前沿进展每日见\n\n文章原文"},{"docId":"article_e85ffb2c-e978-404e-a5e6-37eed89cf724","title":"全给黄仁勋玩明白了！15亿美元租自家GPU/教小弟用GPU换融资，英伟达又一世子被曝准备IPO","link":"http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247823363&idx=1&sn=312d230ee16d03858d86ea0542723a00&chksm=e97a542206b218dbdc095582e5ff482788778662900d53d63185456f8ff17114633bcb330b5d&scene=0#rd","pubDate":"2025.09.05 14:32:00","pubAuthor":"量子位","content":"明敏 发自 凹非寺  \n量子位 | 公众号 QbitAI\n\n英伟达用自家AI芯片，还要从外面租吗？\n\n15亿美元大单，砸向云厂商Lambda。\n\nThe Information消息，今年夏天英伟达将从Lambda租赁1万个装有自家AI芯片的GPU服务器，为期四年，总价值13亿美元。\n\n另外还达成1笔8000个装有英伟达芯片的服务器租赁交易，价值达2亿美元。\n\n租赁目的是用于满足英伟达内部研究和开发需求。\n\n另一边，Lambda被曝正在准备IPO的消息不胫而走。\n\n由此，黄仁勋这一波“左手倒右手”的操作也就弄明白了。\n\n扶持“小弟”是一方面，更重要的是英伟达正在不遗余力加强在云市场的主导性。\n\n是客户、投资者更是供应商\n\n英伟达目前是Lambda的供应商+投资者+客户。\n\n首先在供应方面，Lambda是能最快速提供英伟达最新一代GPU的云厂商之一。\n\n这是一家专注为AI/ML开发提供高性能计算基础设施和服务的公司。核心聚焦于为AI模型训练、微调和部署提供算力支持。相较于大型云厂商，Lambda的GPU租赁价格通常更便宜划算，尤其在长期或者大规模使用场景下。\n\n主要产品包括：\n\n 裸金属GPU云服务：以NVIDIA的高性能GPU为基础的云算力；\n 一键式集群：快速启动多节点GPU训练集群；\n Lambda Inference API：提供托管式推理端点，让企业更便捷地部署模型；\n Lambda Chat：基于开源模型的隐私优先聊天应用，方便直接调用已优化模型；\n Lambda Stack：集成 PyTorch、TensorFlow、CUDA、cuDNN、驱动等工具链的软件环境，便于快速部署和开发；\n 以及NVIDIA DGX Systems\n\n投资方面，英伟达参与了Lambda于2025年2月完成的4.8亿美元D轮融资（股权融资）。在此轮融资中，英伟达作为新的投资者加入，与Andra Capital、SGW等共同领投。\n\n值得一提的是，2024年Lambda还以债务融资的方式完成5亿美元融资。这轮融资主要用于购买数万块英伟达GPU扩大其云基础设施，而购买的GPU将抵押，如果Lambda无法偿还债务，债权人则有权获得这些GPU。能这么做是因为与英伟达签订了协议。\n\n债务融资的好处是能够快速获取大量资本，同时保持股权完整性。对于大规模的硬件采购，债务融资的成本通常低于股权融资；以及这也能加杠杆，如果投资回报率高于借贷成本，债务融资可以放大股东的回报。\n\n最后，英伟达现在也是Lambda的最大客户。\n\n通过这种“循环关系”，英伟达直接推动Lambda在GPU市场的渗透，使其成为AWS、Azure等传统巨头之外“新云”的关键补充。同时也加强和Lambda的合作关系，Lambda扩张得越顺利，英伟达在整体云市场的权重则还能进一步增加。\n\n最关键的是，这种手法已经取得一定效果，号称“英伟达亲儿子”的CoreWeave今年刚刚完成上市。\n\n套路几乎如出一辙，不过CoreWeave的规模更大、英伟达的押注程度也更高。\n\n首先在债务融资上，买卡换资金这种路数，CoreWeave玩得更溜。\n\n通过抵押英伟达GPU的债务融资，是CoreWeave从Magnetar Capital、摩根大通、高盛、摩根士丹利等投资方手中筹集了将近100亿美元。\n\n 2023年8月宣布完成23亿美元债务融资，主要用于扩大GPU基础设施；\n 2024年5月宣布完成75亿美元债务融资，同样用于购买更多英伟达GPU；\n 2024年10月又获得6.5亿美元信贷额度。\n\n另外，英伟达也参与了CoreWeave多轮融资，尤其在2025年CoreWeave上市时，英伟达再次投资约2.5亿美元，巩固其作为战略股东的身份，扩大持有比例。\n\n根据英伟达披露的13F文件，截至今年6月30日，CoreWeave已成为英伟达投资组合中的绝对核心资产。英伟达将91.36%的公开持仓集中押注于AI云计算服务商CoreWeave，总投资额达39.6亿美元。\n\nCoreWeave上市后，在6月中旬前股价一度暴涨400%，市值逼近900亿美元。\n\n不过其Q2财报显示，由于在AI基础设施上的投入过大，其亏损进一步扩大至2.91亿美元，超出市场预期。目前股价在80-90美元之间，是开盘价的2倍以上。\n\n最近Lambda也被曝申请IPO，最早可能在2026年上半年完成。\n\n捍卫算力市场的绝对地位\n\n那么问题就来了，英伟达为啥要费这么大劲扶持小弟？\n\n主要还是为了在云计算市场的竞争。\n\n目前数据中心业务已经成为英伟达最主要的增长引擎，2026财年Q2数据中心业务贡献411亿美元营收，同比增长56%，其中Blackwell平台收入环比增长17%，已进入规模化阶段。\n\n但是为了摆脱来自英伟达的制约，一些大型云厂商已经开始生产自研芯片部署于数据中心，比如AWS和谷歌云。\n\n这对于英伟达的增长构成威胁，为此它们开始进一步深入云市场。\n\n一方面它推出了DGX Cloud服务，这是一个直接向企业提供高性能、完全托管的AI平台。\n\n另一方面就是和“新云”服务商深度绑定，以确保英伟达芯片在市场的渗透率。这对于双方而言是共赢的，英伟达能够保护自身核心业务，小型云厂商也能在英伟达的助力下快速成长。\n\n总之，英伟达还是希望进一步捍卫其在算力市场中绝对霸主的地位。\n\n看来这台“AI印钞机”在老黄的操控下，还会出现更多纪录（骚操作）。\n\n参考链接：  \n[1]https://www.theinformation.com/articles/nvidia-quietly-boosts-cloud-ally-1-5-billion-deal-rent-ai-chips  \n[2]https://techcrunch.com/2025/09/04/cloud-provider-lambda-may-be-gearing-up-for-an-ipo/\n\n一键三连 「点赞」「转发」「小心心」\n\n欢迎在评论区留下你的想法！\n\n— 完 —\n\n专属AI产品从业者的实名社群，只聊AI产品最落地的真问题  扫码添加小助手，发送「姓名+公司+职位」申请入群～\n\n进群后，你将直接获得：\n\n 👉 最新最专业的AI产品信息及分析 🔍 \n\n 👉 不定期发放的热门产品内测码 🔥\n\n 👉 内部专属内容与专业讨论 👂\n\n🌟 点亮星标 🌟\n\n科技前沿进展每日见\n\n文章原文"},{"docId":"article_ac4f2ade-de93-44da-ba96-c6c5100f2df5","title":"第一家被收购的AI浏览器公司，43亿成交，产品还在内测","link":"http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247823363&idx=2&sn=c9b81f5bb5d96fc0357457d704bbc2d5&chksm=e976a1e308c6435ac9fd795e97c10ae6fe7791ab895fe86f91176a54268b8c6c207f93ba8368&scene=0#rd","pubDate":"2025.09.05 14:32:00","pubAuthor":"量子位","content":"henry 发自 凹非寺  \n量子位 | 公众号 QbitAI\n\n全球第一家被收购的AI浏览器公司诞生了！\n\n刚刚，拥有Arc和Dia两款AI浏览器的The Browser Company，被企业协作软件公司Atlassian以6.1亿美元（约为43亿人民币）重金收购。\n\n据悉，此次收购的重点是发布仅三个月且一直处于内测的AI浏览器Dia，Atlassian创始人放话要将其设计成AI时代的白领专业浏览器。\n\n不过，与通常的叫好不同。\n\n这次收购反倒让网友开始质疑Atlassian的眼光——\n\n毕竟，Dia浏览器自6月发布以来至今仍在内测，而且The Browser Company在成立的5年多就没推出过多少像样的产品。\n\n另一方面，也不禁让人感叹，你们AI浏览器都这么值钱的吗？\n\n收购，早有预谋\n\n事实上，这场收购并非像网友想得那么随性，而是早有预谋。\n\n早在一年前，两家CEO就有过联系，并有了收购合并的念头。不过，当时的重点并非Dia，而是公司开发的另一款浏览器——Arc。\n\n（注：Arc因较强的专业性获得了一批死忠用户的跟随，但由于过于陡峭的学习曲线，于去年停止开发）\n\n彼时，有不少Atlassian员工是Arc的用户。他们在对Arc提出产品反馈的同时，也一致地认为The Browser Company可能无法实现他们所预想的产品。\n\n借此机会，Atlassian的创始人Cannon-Brookes就示意The Browser Company的CEO Josh Miller让两家公司合并会更好。\n\n如今，这场交易达成了。\n\n在谈及这场价值6.1亿美元的现金收购时，Josh Miller表示：\n我们将在Atlassian旗下独立运营，并继续开发 Dia 浏览器。\n\n不过，对外界来说，这场收购或多或少有些突然。\n\n为什么这么说呢？\n\n要知道，就在前不久的7月份，《纽约时报》还专门发表专栏，盛赞Dia是一种新的AI浏览器。\n\n而在去年，The Browser Company还以5.5亿美元的估值，拿到了5000万美元的融资。\n\n所以，缺钱是不可能缺钱的。\n\n事实上，自2019年成立以来，The Browser Company已经积累了数百万用户，总计筹集的融资也高达1.28亿美元。\n\n其中，投资者也不乏有PaceCapital、LinkedIn的Jeff Weiner、Medium的EvWilliams、Figma的Dylan Field、Notion的Akshay Kothari以及GitHub的Jason Warner这样的大佬。\n\n所以，为啥好好的就想着被收购了呢？\n\n对于这一问题，Josh Miller表示：AI浏览器领域的胜者将在未来12到24个月内诞生，因此必须在短时间获得庞大的分发渠道、销售团队以及规模化能力。\n\n而这次收购的目的就在于获得所需的资源、推广渠道和变现能力，从而让团队能更快招聘、更快交付，并将Dia推向更多用户。\n\n因此，这场收购实际上是为了帮助The Browser Company在这个极度火热的市场中获得急需的稳定性。\n\n那么，让我们再次把镜头切到Atlassian，Cannon-Brookes为什么要为The Browser Company买单呢？\n\n这个浏览器，有点不一样？\n\n毫无疑问，Atlassian和Cannon-Brookes将赌注压在了The Browser Company的招牌AI浏览器——Dia上。\n\nCannon-Brookes的哲学很简单：如今的浏览器主要为浏览信息而设计，而非为工作效率而生。它无法理解用户的工作任务和优先级，也不能有效连接各类工具，只是在工作流程中充当旁观者。\n\n因此，需要打造一款用于操作而非浏览的浏览器，而Dia就是这一愿景的雏形。\n\n在Cannon-Brookes的介绍下，目前Dia主要有三方面亮点：\n\n首先，它针对白领们日常使用的SaaS应用进行了优化。无论是电子邮件、项目管理工具还是设计应用，Dia的标签页都会提供丰富的上下文信息，帮助推进工作。\n\n其次，它打通了AI技能和个人的工作记忆，可以连接应用程序、标签和任务之间的节点。\n\n最后，它还确保了AI操作的安全性和隐私性。\n\n不难看出，Dia针对目前的SaaS工作流进行了一定的优化，并借由AI打通了孤立的应用、文档节点。\n\n但仅凭这样就能兑现Cannon-Brookes和Josh Miller的期望吗？\n\nDia真如《纽约时报》所说的，是AI浏览器未来的形态吗？\n\n咱们骑驴看唱本——走着瞧。\n\n参考链接\n\n[1]https://www.atlassian.com/blog/announcements/atlassian-acquires-the-browser-company\n\n[2]https://www.atlassian.com/\n\n[3]https://x.com/mcannonbrookes/status/1963578665247412228\n\n[4]https://techcrunch.com/2025/09/04/atlassian-to-buy-arc-developer-the-browser-company-for-610m/\n\n[5]https://www.businesswire.com/news/home/20250904645125/en/Atlassian-Enters-Into-Definitive-Agreement-to-Acquire-The-Browser-Company-of-New-York\n\n[6]https://techcrunch.com/2025/06/11/the-browser-company-launches-its-ai-first-browser-dia-in-beta/\n\n[7]https://www.theverge.com/web/770947/browser-company-arc-dia-acquired-atlassian\n\n一键三连 「点赞」「转发」「小心心」\n\n欢迎在评论区留下你的想法！\n\n— 完 —\n\n专属AI产品从业者的实名社群，只聊AI产品最落地的真问题  扫码添加小助手，发送「姓名+公司+职位」申请入群～\n\n进群后，你将直接获得：\n\n 👉 最新最专业的AI产品信息及分析 🔍 \n\n 👉 不定期发放的热门产品内测码 🔥\n\n 👉 内部专属内容与专业讨论 👂\n\n🌟 点亮星标 🌟\n\n科技前沿进展每日见\n\n文章原文"},{"docId":"article_25a301b4-6af6-445d-9673-4b4f497ecd86","title":"ChatGPT新功能，又干掉一批创业项目","link":"http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247823314&idx=1&sn=5f8d6d5f9425a2bfa50030180bb35033&chksm=e9fbb0500ccc37c29df7af95de26271b6918e31595e6445e1024c68b01e142194622b5206d87&scene=0#rd","pubDate":"2025.09.05 12:28:00","pubAuthor":"量子位","content":"金磊 发自 凹非寺  \n量子位 | 公众号 QbitAI\n\nChatGPT的新功能，终于是千呼万唤始出来了——\n\n分支对话（Conversation Branching）。\n\n这是因为此前有不少网友给OpenAI反馈，称“有时候我想进行1-3个分支对话，而不会让原始对话变得混乱”。\n\n而这个功能的作用也是如其名，现在只需要点击“分支对话”的按钮，就可以在原有对话的基础上“岔个话题”聊天了。\n\n虽然功能不算big，但要知道，之前很多创业公司可都是主打这类分支对话功能。\n\n比如T3.chat，就靠这个功能吸引了不少用户，它的页面上的branch off按钮（嗯，又是一记重拳）。\n\n目前，分支对话功能已经上线，人人都可以用喽\\~\n\n效果如何？\n\n在我们点击“分支对话”之后，对话框会在底部自动分割出来一条线，并标明是“从[某某话题]建立的分支”：\n\n然后我们就可以跳出原来的话题来提问，例如：\nAI Agent最近有什么新进展？\n\n然后我们继续回问一句：\n在这个话题之前我们聊了什么？\n\n可以看到，ChatGPT清晰地记得之前“分支功能”的话题，并询问是否需要将这两个话题的核心内容做合并。\n\n不过虽然这个功能对于团队协作有用（从主线各自分支对话，互不打扰），但网友们似乎还在提出改进的建议。\n\n例如有位网友就提议，目前界面还是不够清晰，能不能分支对话在视觉上搞成这样：\n\n“项目”功能也免费了\n\n除了分支对话之外，ChatGPT还把之前付费用户才能用的“项目”（Project）功能免费开放了。\n\n并且“项目”功能还对文件上传做了不同的限制：\n\n 免费用户（Free）：每个项目最多可上传5个文件。\n Plus / ChatGPT Go / Edu 用户：每个项目最多可上传25个文件。\n Pro / Business / Enterprise 用户：每个项目最多可上传40个文件。\n\n以及用户可以为每个项目自定义颜色和图标，如此一来就可以帮助区分多个项目，提升识别效率。\n\n感兴趣的小伙伴现在就可以试试喽\\~\n\n参考链接：  \n[1]https://x.com/OpenAI/status/1963697012014215181  \n[2]https://x.com/OpenAI/status/1963329936368046111\n\n一键三连 「点赞」「转发」「小心心」\n\n欢迎在评论区留下你的想法！\n\n— 完 —\n\n专属AI产品从业者的实名社群，只聊AI产品最落地的真问题  扫码添加小助手，发送「姓名+公司+职位」申请入群～\n\n进群后，你将直接获得：\n\n 👉 最新最专业的AI产品信息及分析 🔍 \n\n 👉 不定期发放的热门产品内测码 🔥\n\n 👉 内部专属内容与专业讨论 👂\n\n🌟 点亮星标 🌟\n\n科技前沿进展每日见\n\n文章原文"},{"docId":"article_4e38f7cc-7ff3-434a-9d13-5d6669132f6c","title":"字节Seed最新版原生智能体来了！一个模型搞定手机/电脑/浏览器自主操作","link":"http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247823314&idx=2&sn=e8f7e643eb981dcb8cf393ba45192762&chksm=e92f4929805b96174cebf0fcfaf23392a9edf92de62e5df81c9a720c6ef4bcaeda11e4c8e23d&scene=0#rd","pubDate":"2025.09.05 12:28:00","pubAuthor":"量子位","content":"一水 发自 凹非寺  \n量子位 | 公众号 QbitAI\n\nAI自己玩手机、玩电脑、操作网页浏览器……背后还只靠一个模型。\n\n卷王字节Seed又把智能体带到了一个全新level\\~\n\n比如任务是酱紫的：\n编写一个可以转换重量单位的函数。如果原始单位是千克，目标单位是克，并且要转换的值是Mathilde Seigner（法国知名演员）被提名凯撒奖的次数加1，最终结果会是多少？\n\n而通过一系列操作，Agent仅用时35s就得出了答案为4000克。\n\n这就是Seed最新发布的智能体UI-TARS-2，它不止采用了All in one这样的原生设计，而且表现更是优于Claude和OpenAI Agent等竞争对手。\n\n来看团队成员晒出的成绩单，不管是操作电脑/手机，还是使用浏览器、玩15款小游戏，其丝滑程度和完成率现在已经超越一众竞争对手，并进一步接近人类水平。\n\n而且这里还透露了UI-TARS-2背后的关键秘诀：\n一个通过多轮强化学习训练的原生GUI智能体\n\n划重点，多轮强化学习。依靠这一技巧，UI-TARS-2核心解决了“让AI自主操作图形界面（GUI）”的四大难题：\n\n 数据稀缺：以往方法需要上百万级高质量标注数据，成本极高，扩展困难。\n 环境割裂：不同任务（电脑、手机、网页、终端、游戏）通常要在不同框架里训练，无法统一。\n 能力单一：大多数智能体只能做GUI点击或终端命令，难以完成真实复杂任务。\n 训练不稳定：强化学习在GUI任务上容易出现奖励稀疏、策略崩溃，模型很难可靠收敛。\n\n且看字节团队如何见招拆招——\n\n针对四大难题打出“组合拳”\n\n先来看UI-TARS-2的整体设计思路和框架。\n\n其核心目标为：构建一个真正原生、通用、跨平台的GUI智能体，能在电脑、手机、网页、终端，甚至游戏里自主完成复杂任务。\n\n为此，UI-TARS-2架构主要包含下面这几个部分：\n\n1、统一的Agent架构：以大语言模型为核心决策器（Planner），将自然语言指令→GUI/终端/工具操作，全部纳入一个通用执行循环，同时支持GUI点击、键盘输入、终端命令、API调用等多种操作流。\n\n2、多模态感知与环境交互：输入端整合屏幕截图（视觉）+文本上下文+历史操作轨迹，输出端可以是点击坐标、拖拽动作、命令行、甚至API请求，形成感知—决策—执行—反馈的完整闭环。\n\n3、多轮强化学习：不依赖人工标注，而是通过环境反馈奖励优化策略，同时采用模拟环境（sandbox）+自动化任务生成，构建出“数据飞轮”，让模型能不断自我进化。\n\n4、混合操作流（Hybrid Flows）：在一次任务轨迹中，智能体可以无缝组合GUI点击、终端命令和API调用。例如，在网页上找资料（GUI），处理数据（终端），再调用搜索API（工具）。\n\n下面这个让UI-TARS-2玩游戏的例子，很好地展示了这套框架的具体运作过程：\n\n而基于这套框架，UI-TARS-2逐一解决了智能体自主操作GUI面临的四大难题。\n\n靠“数据飞轮”解决数据少的问题\n\n为了打造数据飞轮，团队采用了以下策略：\n\n1）冷启动：这一阶段主要是广泛收集原始数据，以及通过合成数据和人工标注来构建监督微调所需的原始高质量、任务针对性强的数据。\n\n2）多轮迭代：有了数据之后，先让模型通过预训练学习更广泛的知识（如不同GUI操作流程、任务解决思路等），再使用高质量特定任务数据对模型进行优化，最后通过强化学习进行端到端优化。\n\n在每轮迭代中，团队会使用拒绝采样（RFT）或交互标注生成新轨迹，这些轨迹记录了模型在执行任务过程中的推理、动作、环境状态以及反馈等信息，将其按质量筛选后，高质量的进微调数据集，低质量的进预训练数据集。\n\n3）循环增强：然后模型用更新后的数据集重新训练，能力提升后又能生成更好的轨迹，形成“模型变好→数据变好→模型更好”的循环，不断解决数据稀缺问题。\n\n用“多轮强化学习”让AI操作更稳\n\n针对传统强化学习在GUI长周期任务中“奖励稀疏、优化不稳定、信用分配难” 的问题，团队从任务设计、奖励机制、训练流程三方面进行了优化。\n\n如何进行任务奖励设计？团队先对任务类型进行拆分：\n\n对可验证任务（如游戏得分、网页信息检索），用明确信号（成功/失败、LLM对比答案打分）当奖励；\n\n对模糊任务（如复杂GUI操作），训练UI-TARS-2自身作为“奖励模型”，根据轨迹和截图输出任务成功分数，保证奖励可靠。\n\n确定了这一模式后，团队摒弃“等所有任务完成再训练”的模式，转而采用“异步rollout”——\n\n把模型推理环节单独拿出来，和实际执行过程分离，互不干扰；同时只要凑够最少需要的轨迹数量，就立刻用这些完成的轨迹开始训练，那些未完成的就留到下一轮。\n\n等到训练时，团队还在PPO算法上进行了3处优化，从而让模型操作更稳，包括用“解耦GAE”避免长序列价值估计偏差、用“不对称裁剪”鼓励模型尝试那些看似不常用、但可能有效的操作等。\n\n打造“混合操作环境”突破界面限制\n\n为解决纯GUI操作（仅鼠标/键盘）无法应对数据处理、软件开发等复杂工作流的问题，团队构建了“GUI+多工具”融合的交互环境：\n\n不仅整合多操作流，比如在同一环境里，智能体既能做GUI基础操作（点击、输入、滚动网页/APP），又能直接调用终端命令（如用Bash处理文件）、调用API，无需切换上下文。\n\n还为其适配多场景载体，比如在云虚拟机里，内置文件系统、远程VS Code、Jupyter等工具；在浏览器沙箱里，也能关联终端功能，让操作覆盖“桌面-移动-游戏” 全场景。\n\n建“统一沙盒平台”支撑大规模训练\n\n针对传统环境“难复现、易崩溃、吞吐量低”的工程瓶颈，团队打造了兼容多载体的统一沙盒，保障百万级交互训练需求。\n\n简单来说，这就是一个虚拟的模型训练场，支持智能体在里面大规模练习、试错和进化。\n\n以下为一个浏览器沙盒的示意图，据此也能看到沙盒的大致运作方式：\n\n优于Claude和OpenAI Agent等竞争对手\n\n那么，UI-TARS-2的实际表现如何呢？\n\n根据介绍，UI-TARS-2是团队基于Seed-thinking-1.6（总参数230B，含532M视觉编码器 + 23B激活参数），经过多轮迭代训练而成。\n\n在多个权威GUI测试里，比如OSWorld（369个Windows/Ubuntu/macOS任务）、WindowsAgentArena（150个 Windows任务）、TerminalBench（命令行任务）等，它的得分都比Claude、OpenAI的同类模型更高。\n\n换句话说，在电脑、手机、浏览器操作上，UI-TARS-2的表现明显更好。\n\n而且在15款小游戏（比如2048、拼图、迷宫）里，它的平均得分差不多是人类水平的60%，比OpenAI、Claude的游戏AI强不少，有的游戏（如“Shapes”）甚至比人玩得还好。\n\n即使面对一些更复杂的游戏测试（LMGame-Bench），它也能和o3打得有来有回。\n\n当然了，它不光会点界面、玩游戏，还能干“查资料”、“写代码修bug” 这样的活儿，而且成绩比只靠界面操作强很多。\n\n总之，UI-TARS-2无疑验证了多轮强化学习在智能体进化上的有效性。\n\n论文：  \nhttps://arxiv.org/abs/2509.02544  \ndemo：  \nhttps://seed-tars.com/showcase/ui-tars-2/\n\n参考链接：  \n[1]https://x.com/TsingYoga/status/1963629621326614940  \n[2]https://x.com/\\akhaliq/status/1963229296236937443\n\n一键三连 「点赞」「转发」「小心心」\n\n欢迎在评论区留下你的想法！\n\n— 完 —\n\n专属AI产品从业者的实名社群，只聊AI产品最落地的真问题  扫码添加小助手，发送「姓名+公司+职位」申请入群～\n\n进群后，你将直接获得：\n\n 👉 最新最专业的AI产品信息及分析 🔍 \n\n 👉 不定期发放的热门产品内测码 🔥\n\n 👉 内部专属内容与专业讨论 👂\n\n🌟 点亮星标 🌟\n\n科技前沿进展每日见\n\n文章原文"}]